{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rede Neural (MLP)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "DSBCAtx4HUwk",
        "NUmALLG1HLCh",
        "Guv55saT2XtC",
        "iVgv74k-2gvR",
        "HDjpMj6a2g_j",
        "7XlmdBn_2hCL",
        "xyhaY1zK2hHn",
        "19kIhS3Z2hLn",
        "5ttzrGER2qof",
        "He-QjpuH2rG_"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSBCAtx4HUwk",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFbYUHl0U3bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmALLG1HLCh",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pXOPI3shyZx",
        "colab_type": "code",
        "outputId": "7bcebe08-813b-4fe6-a457-a2befea6c7a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "len(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgyU_na6iRQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "X_test = StandardScaler().fit_transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u07CaB36HOlP",
        "colab_type": "text"
      },
      "source": [
        "# Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDknWMIkmzpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=1000, solver='adam', verbose=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZMVOJuItGv1",
        "colab_type": "code",
        "outputId": "12c0fe4d-2e80-4d32-be6b-baaa6e1a2112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9928
        }
      },
      "source": [
        "mlp.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.16201234\n",
            "Iteration 2, loss = 1.14694063\n",
            "Iteration 3, loss = 1.13232753\n",
            "Iteration 4, loss = 1.11818314\n",
            "Iteration 5, loss = 1.10441148\n",
            "Iteration 6, loss = 1.09095305\n",
            "Iteration 7, loss = 1.07782809\n",
            "Iteration 8, loss = 1.06506511\n",
            "Iteration 9, loss = 1.05276943\n",
            "Iteration 10, loss = 1.04086335\n",
            "Iteration 11, loss = 1.02925073\n",
            "Iteration 12, loss = 1.01789410\n",
            "Iteration 13, loss = 1.00680149\n",
            "Iteration 14, loss = 0.99588302\n",
            "Iteration 15, loss = 0.98511189\n",
            "Iteration 16, loss = 0.97451371\n",
            "Iteration 17, loss = 0.96409689\n",
            "Iteration 18, loss = 0.95382700\n",
            "Iteration 19, loss = 0.94373810\n",
            "Iteration 20, loss = 0.93375187\n",
            "Iteration 21, loss = 0.92390355\n",
            "Iteration 22, loss = 0.91424367\n",
            "Iteration 23, loss = 0.90468454\n",
            "Iteration 24, loss = 0.89518118\n",
            "Iteration 25, loss = 0.88571736\n",
            "Iteration 26, loss = 0.87633298\n",
            "Iteration 27, loss = 0.86707331\n",
            "Iteration 28, loss = 0.85796804\n",
            "Iteration 29, loss = 0.84895491\n",
            "Iteration 30, loss = 0.83995757\n",
            "Iteration 31, loss = 0.83104256\n",
            "Iteration 32, loss = 0.82219196\n",
            "Iteration 33, loss = 0.81344226\n",
            "Iteration 34, loss = 0.80474476\n",
            "Iteration 35, loss = 0.79611096\n",
            "Iteration 36, loss = 0.78757805\n",
            "Iteration 37, loss = 0.77912257\n",
            "Iteration 38, loss = 0.77070906\n",
            "Iteration 39, loss = 0.76235189\n",
            "Iteration 40, loss = 0.75406536\n",
            "Iteration 41, loss = 0.74582693\n",
            "Iteration 42, loss = 0.73764538\n",
            "Iteration 43, loss = 0.72952220\n",
            "Iteration 44, loss = 0.72145458\n",
            "Iteration 45, loss = 0.71345718\n",
            "Iteration 46, loss = 0.70562549\n",
            "Iteration 47, loss = 0.69799932\n",
            "Iteration 48, loss = 0.69059284\n",
            "Iteration 49, loss = 0.68327139\n",
            "Iteration 50, loss = 0.67600426\n",
            "Iteration 51, loss = 0.66881348\n",
            "Iteration 52, loss = 0.66178510\n",
            "Iteration 53, loss = 0.65479167\n",
            "Iteration 54, loss = 0.64778934\n",
            "Iteration 55, loss = 0.64079651\n",
            "Iteration 56, loss = 0.63382996\n",
            "Iteration 57, loss = 0.62691215\n",
            "Iteration 58, loss = 0.62001203\n",
            "Iteration 59, loss = 0.61315183\n",
            "Iteration 60, loss = 0.60634776\n",
            "Iteration 61, loss = 0.59953481\n",
            "Iteration 62, loss = 0.59275189\n",
            "Iteration 63, loss = 0.58605521\n",
            "Iteration 64, loss = 0.57941080\n",
            "Iteration 65, loss = 0.57280851\n",
            "Iteration 66, loss = 0.56628429\n",
            "Iteration 67, loss = 0.55981165\n",
            "Iteration 68, loss = 0.55339582\n",
            "Iteration 69, loss = 0.54708133\n",
            "Iteration 70, loss = 0.54086441\n",
            "Iteration 71, loss = 0.53472528\n",
            "Iteration 72, loss = 0.52867210\n",
            "Iteration 73, loss = 0.52271211\n",
            "Iteration 74, loss = 0.51684368\n",
            "Iteration 75, loss = 0.51105523\n",
            "Iteration 76, loss = 0.50537324\n",
            "Iteration 77, loss = 0.49978805\n",
            "Iteration 78, loss = 0.49426586\n",
            "Iteration 79, loss = 0.48880743\n",
            "Iteration 80, loss = 0.48345300\n",
            "Iteration 81, loss = 0.47817530\n",
            "Iteration 82, loss = 0.47296897\n",
            "Iteration 83, loss = 0.46789389\n",
            "Iteration 84, loss = 0.46292908\n",
            "Iteration 85, loss = 0.45809212\n",
            "Iteration 86, loss = 0.45338224\n",
            "Iteration 87, loss = 0.44879033\n",
            "Iteration 88, loss = 0.44431096\n",
            "Iteration 89, loss = 0.43995522\n",
            "Iteration 90, loss = 0.43571074\n",
            "Iteration 91, loss = 0.43158684\n",
            "Iteration 92, loss = 0.42761333\n",
            "Iteration 93, loss = 0.42377486\n",
            "Iteration 94, loss = 0.42006043\n",
            "Iteration 95, loss = 0.41647922\n",
            "Iteration 96, loss = 0.41300141\n",
            "Iteration 97, loss = 0.40961303\n",
            "Iteration 98, loss = 0.40630993\n",
            "Iteration 99, loss = 0.40309893\n",
            "Iteration 100, loss = 0.39999911\n",
            "Iteration 101, loss = 0.39699418\n",
            "Iteration 102, loss = 0.39408333\n",
            "Iteration 103, loss = 0.39125258\n",
            "Iteration 104, loss = 0.38849084\n",
            "Iteration 105, loss = 0.38578925\n",
            "Iteration 106, loss = 0.38315405\n",
            "Iteration 107, loss = 0.38057732\n",
            "Iteration 108, loss = 0.37805910\n",
            "Iteration 109, loss = 0.37559886\n",
            "Iteration 110, loss = 0.37319418\n",
            "Iteration 111, loss = 0.37083749\n",
            "Iteration 112, loss = 0.36852713\n",
            "Iteration 113, loss = 0.36625825\n",
            "Iteration 114, loss = 0.36403139\n",
            "Iteration 115, loss = 0.36184401\n",
            "Iteration 116, loss = 0.35969134\n",
            "Iteration 117, loss = 0.35756880\n",
            "Iteration 118, loss = 0.35547587\n",
            "Iteration 119, loss = 0.35340720\n",
            "Iteration 120, loss = 0.35136573\n",
            "Iteration 121, loss = 0.34934341\n",
            "Iteration 122, loss = 0.34732928\n",
            "Iteration 123, loss = 0.34533273\n",
            "Iteration 124, loss = 0.34335488\n",
            "Iteration 125, loss = 0.34138724\n",
            "Iteration 126, loss = 0.33942831\n",
            "Iteration 127, loss = 0.33750783\n",
            "Iteration 128, loss = 0.33563111\n",
            "Iteration 129, loss = 0.33377666\n",
            "Iteration 130, loss = 0.33194273\n",
            "Iteration 131, loss = 0.33011696\n",
            "Iteration 132, loss = 0.32831174\n",
            "Iteration 133, loss = 0.32652115\n",
            "Iteration 134, loss = 0.32474809\n",
            "Iteration 135, loss = 0.32298404\n",
            "Iteration 136, loss = 0.32122399\n",
            "Iteration 137, loss = 0.31948029\n",
            "Iteration 138, loss = 0.31775298\n",
            "Iteration 139, loss = 0.31602863\n",
            "Iteration 140, loss = 0.31431177\n",
            "Iteration 141, loss = 0.31259875\n",
            "Iteration 142, loss = 0.31087314\n",
            "Iteration 143, loss = 0.30913487\n",
            "Iteration 144, loss = 0.30737548\n",
            "Iteration 145, loss = 0.30560507\n",
            "Iteration 146, loss = 0.30382279\n",
            "Iteration 147, loss = 0.30206598\n",
            "Iteration 148, loss = 0.30032866\n",
            "Iteration 149, loss = 0.29858791\n",
            "Iteration 150, loss = 0.29683735\n",
            "Iteration 151, loss = 0.29509607\n",
            "Iteration 152, loss = 0.29336591\n",
            "Iteration 153, loss = 0.29166067\n",
            "Iteration 154, loss = 0.28996975\n",
            "Iteration 155, loss = 0.28827612\n",
            "Iteration 156, loss = 0.28658085\n",
            "Iteration 157, loss = 0.28489060\n",
            "Iteration 158, loss = 0.28321429\n",
            "Iteration 159, loss = 0.28155220\n",
            "Iteration 160, loss = 0.27989170\n",
            "Iteration 161, loss = 0.27822950\n",
            "Iteration 162, loss = 0.27656893\n",
            "Iteration 163, loss = 0.27490776\n",
            "Iteration 164, loss = 0.27324949\n",
            "Iteration 165, loss = 0.27159117\n",
            "Iteration 166, loss = 0.26993429\n",
            "Iteration 167, loss = 0.26827512\n",
            "Iteration 168, loss = 0.26661371\n",
            "Iteration 169, loss = 0.26495137\n",
            "Iteration 170, loss = 0.26328319\n",
            "Iteration 171, loss = 0.26159099\n",
            "Iteration 172, loss = 0.25989457\n",
            "Iteration 173, loss = 0.25819577\n",
            "Iteration 174, loss = 0.25648553\n",
            "Iteration 175, loss = 0.25475230\n",
            "Iteration 176, loss = 0.25300184\n",
            "Iteration 177, loss = 0.25122285\n",
            "Iteration 178, loss = 0.24942976\n",
            "Iteration 179, loss = 0.24761635\n",
            "Iteration 180, loss = 0.24578593\n",
            "Iteration 181, loss = 0.24393444\n",
            "Iteration 182, loss = 0.24207040\n",
            "Iteration 183, loss = 0.24019267\n",
            "Iteration 184, loss = 0.23830532\n",
            "Iteration 185, loss = 0.23640090\n",
            "Iteration 186, loss = 0.23449021\n",
            "Iteration 187, loss = 0.23258475\n",
            "Iteration 188, loss = 0.23068466\n",
            "Iteration 189, loss = 0.22877932\n",
            "Iteration 190, loss = 0.22686649\n",
            "Iteration 191, loss = 0.22495056\n",
            "Iteration 192, loss = 0.22301131\n",
            "Iteration 193, loss = 0.22105408\n",
            "Iteration 194, loss = 0.21908963\n",
            "Iteration 195, loss = 0.21712173\n",
            "Iteration 196, loss = 0.21516751\n",
            "Iteration 197, loss = 0.21321879\n",
            "Iteration 198, loss = 0.21120796\n",
            "Iteration 199, loss = 0.20916544\n",
            "Iteration 200, loss = 0.20709148\n",
            "Iteration 201, loss = 0.20500213\n",
            "Iteration 202, loss = 0.20290756\n",
            "Iteration 203, loss = 0.20081366\n",
            "Iteration 204, loss = 0.19870957\n",
            "Iteration 205, loss = 0.19658371\n",
            "Iteration 206, loss = 0.19444719\n",
            "Iteration 207, loss = 0.19230789\n",
            "Iteration 208, loss = 0.19016259\n",
            "Iteration 209, loss = 0.18802073\n",
            "Iteration 210, loss = 0.18589707\n",
            "Iteration 211, loss = 0.18377959\n",
            "Iteration 212, loss = 0.18167333\n",
            "Iteration 213, loss = 0.17959211\n",
            "Iteration 214, loss = 0.17752483\n",
            "Iteration 215, loss = 0.17548420\n",
            "Iteration 216, loss = 0.17346013\n",
            "Iteration 217, loss = 0.17143985\n",
            "Iteration 218, loss = 0.16943155\n",
            "Iteration 219, loss = 0.16745817\n",
            "Iteration 220, loss = 0.16551769\n",
            "Iteration 221, loss = 0.16359715\n",
            "Iteration 222, loss = 0.16169491\n",
            "Iteration 223, loss = 0.15981087\n",
            "Iteration 224, loss = 0.15794253\n",
            "Iteration 225, loss = 0.15609253\n",
            "Iteration 226, loss = 0.15426051\n",
            "Iteration 227, loss = 0.15244761\n",
            "Iteration 228, loss = 0.15065739\n",
            "Iteration 229, loss = 0.14888968\n",
            "Iteration 230, loss = 0.14714666\n",
            "Iteration 231, loss = 0.14543104\n",
            "Iteration 232, loss = 0.14373878\n",
            "Iteration 233, loss = 0.14207217\n",
            "Iteration 234, loss = 0.14042690\n",
            "Iteration 235, loss = 0.13880874\n",
            "Iteration 236, loss = 0.13722059\n",
            "Iteration 237, loss = 0.13566681\n",
            "Iteration 238, loss = 0.13414963\n",
            "Iteration 239, loss = 0.13265640\n",
            "Iteration 240, loss = 0.13119833\n",
            "Iteration 241, loss = 0.12977543\n",
            "Iteration 242, loss = 0.12838311\n",
            "Iteration 243, loss = 0.12701958\n",
            "Iteration 244, loss = 0.12568923\n",
            "Iteration 245, loss = 0.12439123\n",
            "Iteration 246, loss = 0.12311595\n",
            "Iteration 247, loss = 0.12184758\n",
            "Iteration 248, loss = 0.12060197\n",
            "Iteration 249, loss = 0.11938477\n",
            "Iteration 250, loss = 0.11818638\n",
            "Iteration 251, loss = 0.11700139\n",
            "Iteration 252, loss = 0.11583038\n",
            "Iteration 253, loss = 0.11467431\n",
            "Iteration 254, loss = 0.11353363\n",
            "Iteration 255, loss = 0.11241055\n",
            "Iteration 256, loss = 0.11130288\n",
            "Iteration 257, loss = 0.11022177\n",
            "Iteration 258, loss = 0.10917832\n",
            "Iteration 259, loss = 0.10815297\n",
            "Iteration 260, loss = 0.10714035\n",
            "Iteration 261, loss = 0.10611951\n",
            "Iteration 262, loss = 0.10508494\n",
            "Iteration 263, loss = 0.10406129\n",
            "Iteration 264, loss = 0.10304492\n",
            "Iteration 265, loss = 0.10203852\n",
            "Iteration 266, loss = 0.10104280\n",
            "Iteration 267, loss = 0.10005529\n",
            "Iteration 268, loss = 0.09907598\n",
            "Iteration 269, loss = 0.09810338\n",
            "Iteration 270, loss = 0.09713856\n",
            "Iteration 271, loss = 0.09618719\n",
            "Iteration 272, loss = 0.09524673\n",
            "Iteration 273, loss = 0.09431701\n",
            "Iteration 274, loss = 0.09339916\n",
            "Iteration 275, loss = 0.09248745\n",
            "Iteration 276, loss = 0.09158331\n",
            "Iteration 277, loss = 0.09068701\n",
            "Iteration 278, loss = 0.08979990\n",
            "Iteration 279, loss = 0.08892277\n",
            "Iteration 280, loss = 0.08805384\n",
            "Iteration 281, loss = 0.08719637\n",
            "Iteration 282, loss = 0.08638539\n",
            "Iteration 283, loss = 0.08558823\n",
            "Iteration 284, loss = 0.08480195\n",
            "Iteration 285, loss = 0.08402679\n",
            "Iteration 286, loss = 0.08326043\n",
            "Iteration 287, loss = 0.08250717\n",
            "Iteration 288, loss = 0.08176051\n",
            "Iteration 289, loss = 0.08102274\n",
            "Iteration 290, loss = 0.08029381\n",
            "Iteration 291, loss = 0.07957407\n",
            "Iteration 292, loss = 0.07886359\n",
            "Iteration 293, loss = 0.07816175\n",
            "Iteration 294, loss = 0.07745859\n",
            "Iteration 295, loss = 0.07675633\n",
            "Iteration 296, loss = 0.07604883\n",
            "Iteration 297, loss = 0.07533287\n",
            "Iteration 298, loss = 0.07461578\n",
            "Iteration 299, loss = 0.07384907\n",
            "Iteration 300, loss = 0.07306557\n",
            "Iteration 301, loss = 0.07229620\n",
            "Iteration 302, loss = 0.07152236\n",
            "Iteration 303, loss = 0.07076203\n",
            "Iteration 304, loss = 0.07005936\n",
            "Iteration 305, loss = 0.06939907\n",
            "Iteration 306, loss = 0.06874025\n",
            "Iteration 307, loss = 0.06808749\n",
            "Iteration 308, loss = 0.06744179\n",
            "Iteration 309, loss = 0.06680547\n",
            "Iteration 310, loss = 0.06617545\n",
            "Iteration 311, loss = 0.06555645\n",
            "Iteration 312, loss = 0.06494335\n",
            "Iteration 313, loss = 0.06433341\n",
            "Iteration 314, loss = 0.06373167\n",
            "Iteration 315, loss = 0.06313886\n",
            "Iteration 316, loss = 0.06255451\n",
            "Iteration 317, loss = 0.06197828\n",
            "Iteration 318, loss = 0.06141249\n",
            "Iteration 319, loss = 0.06085371\n",
            "Iteration 320, loss = 0.06030371\n",
            "Iteration 321, loss = 0.05976282\n",
            "Iteration 322, loss = 0.05923294\n",
            "Iteration 323, loss = 0.05871120\n",
            "Iteration 324, loss = 0.05819703\n",
            "Iteration 325, loss = 0.05769996\n",
            "Iteration 326, loss = 0.05720618\n",
            "Iteration 327, loss = 0.05671866\n",
            "Iteration 328, loss = 0.05623723\n",
            "Iteration 329, loss = 0.05576301\n",
            "Iteration 330, loss = 0.05529567\n",
            "Iteration 331, loss = 0.05483959\n",
            "Iteration 332, loss = 0.05438708\n",
            "Iteration 333, loss = 0.05394007\n",
            "Iteration 334, loss = 0.05349904\n",
            "Iteration 335, loss = 0.05306218\n",
            "Iteration 336, loss = 0.05263091\n",
            "Iteration 337, loss = 0.05220558\n",
            "Iteration 338, loss = 0.05178540\n",
            "Iteration 339, loss = 0.05137984\n",
            "Iteration 340, loss = 0.05097758\n",
            "Iteration 341, loss = 0.05057761\n",
            "Iteration 342, loss = 0.05018436\n",
            "Iteration 343, loss = 0.04979704\n",
            "Iteration 344, loss = 0.04941284\n",
            "Iteration 345, loss = 0.04903765\n",
            "Iteration 346, loss = 0.04867355\n",
            "Iteration 347, loss = 0.04831253\n",
            "Iteration 348, loss = 0.04795294\n",
            "Iteration 349, loss = 0.04760307\n",
            "Iteration 350, loss = 0.04725839\n",
            "Iteration 351, loss = 0.04692091\n",
            "Iteration 352, loss = 0.04659122\n",
            "Iteration 353, loss = 0.04626149\n",
            "Iteration 354, loss = 0.04593719\n",
            "Iteration 355, loss = 0.04561524\n",
            "Iteration 356, loss = 0.04530075\n",
            "Iteration 357, loss = 0.04498870\n",
            "Iteration 358, loss = 0.04468609\n",
            "Iteration 359, loss = 0.04438220\n",
            "Iteration 360, loss = 0.04408281\n",
            "Iteration 361, loss = 0.04378956\n",
            "Iteration 362, loss = 0.04349750\n",
            "Iteration 363, loss = 0.04320988\n",
            "Iteration 364, loss = 0.04292865\n",
            "Iteration 365, loss = 0.04264633\n",
            "Iteration 366, loss = 0.04236467\n",
            "Iteration 367, loss = 0.04209053\n",
            "Iteration 368, loss = 0.04181722\n",
            "Iteration 369, loss = 0.04154866\n",
            "Iteration 370, loss = 0.04128244\n",
            "Iteration 371, loss = 0.04101891\n",
            "Iteration 372, loss = 0.04076177\n",
            "Iteration 373, loss = 0.04050420\n",
            "Iteration 374, loss = 0.04024607\n",
            "Iteration 375, loss = 0.03998825\n",
            "Iteration 376, loss = 0.03974024\n",
            "Iteration 377, loss = 0.03949033\n",
            "Iteration 378, loss = 0.03923910\n",
            "Iteration 379, loss = 0.03899525\n",
            "Iteration 380, loss = 0.03875216\n",
            "Iteration 381, loss = 0.03851260\n",
            "Iteration 382, loss = 0.03827158\n",
            "Iteration 383, loss = 0.03802872\n",
            "Iteration 384, loss = 0.03778494\n",
            "Iteration 385, loss = 0.03754544\n",
            "Iteration 386, loss = 0.03731229\n",
            "Iteration 387, loss = 0.03707822\n",
            "Iteration 388, loss = 0.03684933\n",
            "Iteration 389, loss = 0.03661864\n",
            "Iteration 390, loss = 0.03639806\n",
            "Iteration 391, loss = 0.03619314\n",
            "Iteration 392, loss = 0.03598690\n",
            "Iteration 393, loss = 0.03578297\n",
            "Iteration 394, loss = 0.03557021\n",
            "Iteration 395, loss = 0.03536004\n",
            "Iteration 396, loss = 0.03514888\n",
            "Iteration 397, loss = 0.03493804\n",
            "Iteration 398, loss = 0.03472600\n",
            "Iteration 399, loss = 0.03451109\n",
            "Iteration 400, loss = 0.03431609\n",
            "Iteration 401, loss = 0.03411779\n",
            "Iteration 402, loss = 0.03391275\n",
            "Iteration 403, loss = 0.03371143\n",
            "Iteration 404, loss = 0.03351024\n",
            "Iteration 405, loss = 0.03331234\n",
            "Iteration 406, loss = 0.03312151\n",
            "Iteration 407, loss = 0.03292708\n",
            "Iteration 408, loss = 0.03273248\n",
            "Iteration 409, loss = 0.03253572\n",
            "Iteration 410, loss = 0.03234828\n",
            "Iteration 411, loss = 0.03216138\n",
            "Iteration 412, loss = 0.03197287\n",
            "Iteration 413, loss = 0.03178293\n",
            "Iteration 414, loss = 0.03159220\n",
            "Iteration 415, loss = 0.03140143\n",
            "Iteration 416, loss = 0.03122161\n",
            "Iteration 417, loss = 0.03103878\n",
            "Iteration 418, loss = 0.03084914\n",
            "Iteration 419, loss = 0.03068012\n",
            "Iteration 420, loss = 0.03051088\n",
            "Iteration 421, loss = 0.03034243\n",
            "Iteration 422, loss = 0.03016997\n",
            "Iteration 423, loss = 0.02999407\n",
            "Iteration 424, loss = 0.02981653\n",
            "Iteration 425, loss = 0.02964417\n",
            "Iteration 426, loss = 0.02947283\n",
            "Iteration 427, loss = 0.02930849\n",
            "Iteration 428, loss = 0.02914583\n",
            "Iteration 429, loss = 0.02897937\n",
            "Iteration 430, loss = 0.02880917\n",
            "Iteration 431, loss = 0.02864406\n",
            "Iteration 432, loss = 0.02847236\n",
            "Iteration 433, loss = 0.02830493\n",
            "Iteration 434, loss = 0.02814405\n",
            "Iteration 435, loss = 0.02798015\n",
            "Iteration 436, loss = 0.02783121\n",
            "Iteration 437, loss = 0.02767660\n",
            "Iteration 438, loss = 0.02751405\n",
            "Iteration 439, loss = 0.02736137\n",
            "Iteration 440, loss = 0.02720301\n",
            "Iteration 441, loss = 0.02704496\n",
            "Iteration 442, loss = 0.02688372\n",
            "Iteration 443, loss = 0.02672972\n",
            "Iteration 444, loss = 0.02657514\n",
            "Iteration 445, loss = 0.02641767\n",
            "Iteration 446, loss = 0.02627350\n",
            "Iteration 447, loss = 0.02611899\n",
            "Iteration 448, loss = 0.02597370\n",
            "Iteration 449, loss = 0.02582205\n",
            "Iteration 450, loss = 0.02566589\n",
            "Iteration 451, loss = 0.02551834\n",
            "Iteration 452, loss = 0.02537321\n",
            "Iteration 453, loss = 0.02522410\n",
            "Iteration 454, loss = 0.02508366\n",
            "Iteration 455, loss = 0.02494003\n",
            "Iteration 456, loss = 0.02479752\n",
            "Iteration 457, loss = 0.02464470\n",
            "Iteration 458, loss = 0.02450558\n",
            "Iteration 459, loss = 0.02436793\n",
            "Iteration 460, loss = 0.02422837\n",
            "Iteration 461, loss = 0.02408552\n",
            "Iteration 462, loss = 0.02394085\n",
            "Iteration 463, loss = 0.02380245\n",
            "Iteration 464, loss = 0.02366170\n",
            "Iteration 465, loss = 0.02352302\n",
            "Iteration 466, loss = 0.02339046\n",
            "Iteration 467, loss = 0.02325239\n",
            "Iteration 468, loss = 0.02311894\n",
            "Iteration 469, loss = 0.02297990\n",
            "Iteration 470, loss = 0.02284202\n",
            "Iteration 471, loss = 0.02271674\n",
            "Iteration 472, loss = 0.02259044\n",
            "Iteration 473, loss = 0.02245787\n",
            "Iteration 474, loss = 0.02232122\n",
            "Iteration 475, loss = 0.02217979\n",
            "Iteration 476, loss = 0.02204359\n",
            "Iteration 477, loss = 0.02192756\n",
            "Iteration 478, loss = 0.02179976\n",
            "Iteration 479, loss = 0.02166153\n",
            "Iteration 480, loss = 0.02153417\n",
            "Iteration 481, loss = 0.02140881\n",
            "Iteration 482, loss = 0.02127968\n",
            "Iteration 483, loss = 0.02114826\n",
            "Iteration 484, loss = 0.02103613\n",
            "Iteration 485, loss = 0.02091199\n",
            "Iteration 486, loss = 0.02077160\n",
            "Iteration 487, loss = 0.02065948\n",
            "Iteration 488, loss = 0.02053995\n",
            "Iteration 489, loss = 0.02041105\n",
            "Iteration 490, loss = 0.02029044\n",
            "Iteration 491, loss = 0.02017498\n",
            "Iteration 492, loss = 0.02004880\n",
            "Iteration 493, loss = 0.01993000\n",
            "Iteration 494, loss = 0.01981522\n",
            "Iteration 495, loss = 0.01969840\n",
            "Iteration 496, loss = 0.01958039\n",
            "Iteration 497, loss = 0.01946267\n",
            "Iteration 498, loss = 0.01934105\n",
            "Iteration 499, loss = 0.01922764\n",
            "Iteration 500, loss = 0.01911677\n",
            "Iteration 501, loss = 0.01900254\n",
            "Iteration 502, loss = 0.01888522\n",
            "Iteration 503, loss = 0.01877723\n",
            "Iteration 504, loss = 0.01866819\n",
            "Iteration 505, loss = 0.01855414\n",
            "Iteration 506, loss = 0.01843953\n",
            "Iteration 507, loss = 0.01832990\n",
            "Iteration 508, loss = 0.01821967\n",
            "Iteration 509, loss = 0.01811975\n",
            "Iteration 510, loss = 0.01800714\n",
            "Iteration 511, loss = 0.01789276\n",
            "Iteration 512, loss = 0.01778790\n",
            "Iteration 513, loss = 0.01767828\n",
            "Iteration 514, loss = 0.01757789\n",
            "Iteration 515, loss = 0.01747191\n",
            "Iteration 516, loss = 0.01736760\n",
            "Iteration 517, loss = 0.01726747\n",
            "Iteration 518, loss = 0.01716405\n",
            "Iteration 519, loss = 0.01705767\n",
            "Iteration 520, loss = 0.01696223\n",
            "Iteration 521, loss = 0.01686192\n",
            "Iteration 522, loss = 0.01675678\n",
            "Iteration 523, loss = 0.01665459\n",
            "Iteration 524, loss = 0.01656101\n",
            "Iteration 525, loss = 0.01646057\n",
            "Iteration 526, loss = 0.01635681\n",
            "Iteration 527, loss = 0.01625950\n",
            "Iteration 528, loss = 0.01616152\n",
            "Iteration 529, loss = 0.01607085\n",
            "Iteration 530, loss = 0.01597653\n",
            "Iteration 531, loss = 0.01587763\n",
            "Iteration 532, loss = 0.01578333\n",
            "Iteration 533, loss = 0.01568031\n",
            "Iteration 534, loss = 0.01557649\n",
            "Iteration 535, loss = 0.01547834\n",
            "Iteration 536, loss = 0.01537318\n",
            "Iteration 537, loss = 0.01527162\n",
            "Iteration 538, loss = 0.01516989\n",
            "Iteration 539, loss = 0.01506346\n",
            "Iteration 540, loss = 0.01496301\n",
            "Iteration 541, loss = 0.01486052\n",
            "Iteration 542, loss = 0.01475823\n",
            "Iteration 543, loss = 0.01465727\n",
            "Iteration 544, loss = 0.01455376\n",
            "Iteration 545, loss = 0.01445452\n",
            "Iteration 546, loss = 0.01435523\n",
            "Iteration 547, loss = 0.01425251\n",
            "Iteration 548, loss = 0.01414880\n",
            "Iteration 549, loss = 0.01404933\n",
            "Iteration 550, loss = 0.01395095\n",
            "Iteration 551, loss = 0.01385442\n",
            "Iteration 552, loss = 0.01375676\n",
            "Iteration 553, loss = 0.01365690\n",
            "Iteration 554, loss = 0.01356399\n",
            "Iteration 555, loss = 0.01346862\n",
            "Iteration 556, loss = 0.01336725\n",
            "Iteration 557, loss = 0.01327098\n",
            "Iteration 558, loss = 0.01317476\n",
            "Iteration 559, loss = 0.01307545\n",
            "Iteration 560, loss = 0.01297574\n",
            "Iteration 561, loss = 0.01288461\n",
            "Iteration 562, loss = 0.01279186\n",
            "Iteration 563, loss = 0.01269203\n",
            "Iteration 564, loss = 0.01258861\n",
            "Iteration 565, loss = 0.01249276\n",
            "Iteration 566, loss = 0.01239703\n",
            "Iteration 567, loss = 0.01230128\n",
            "Iteration 568, loss = 0.01220562\n",
            "Iteration 569, loss = 0.01211185\n",
            "Iteration 570, loss = 0.01201614\n",
            "Iteration 571, loss = 0.01192209\n",
            "Iteration 572, loss = 0.01182937\n",
            "Iteration 573, loss = 0.01173505\n",
            "Iteration 574, loss = 0.01164620\n",
            "Iteration 575, loss = 0.01155859\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(20, 20, 20), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
              "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
              "              validation_fraction=0.1, verbose=10, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Sp2QhunH14",
        "colab_type": "code",
        "outputId": "de9b264a-c656-40c0-8131-836c6e277619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "predictions = mlp.predict(X_test)\n",
        "predictions\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 2,\n",
              "       0, 0, 2, 0, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAC7ugaf1ACx",
        "colab_type": "text"
      },
      "source": [
        "## Acuracia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od4q1HarzFLt",
        "colab_type": "code",
        "outputId": "d8e03c36-a426-424f-b677-1329825421ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9210526315789473"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGsfaVNBzWLC",
        "colab_type": "text"
      },
      "source": [
        "## Matriz de Confus達o"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly7c5JtWzaa5",
        "colab_type": "code",
        "outputId": "c9f684fc-e234-403c-ef87-ea9bff7fb3ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[13,  0,  0],\n",
              "       [ 0, 13,  3],\n",
              "       [ 0,  0,  9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAmwQW_KzyTh",
        "colab_type": "code",
        "outputId": "cf73494e-cdff-4e02-badf-2de41e2da58f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "sns.heatmap(cm, center=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADu5JREFUeJzt3X/sXXV9x/HXix+aDHAixFILwoYM\nA25i1tQZDasyEBpYTcYfdImgY/sOoxkkSzbcgmSYLFuWsJiwrfnGNuKCdZtY7RZAGibBbcIoTVXa\nokDDQjukEQiFaSK939f++J6mly/3fu/53h+953y+zwc5+Z57zrn3fHJDXn3nfT7nXCcRAGD6jpv2\nAAAA8whkAGgIAhkAGoJABoCGIJABoCEIZABoCAIZABqCQAaAhiCQAaAhTpj4GXbfza2AE3buVbdM\newjAWDy9b49H/pClZM6FvzP6+caIChkAGoJABoCGmHzLAgCOoXQ6tY9tVL9CBDKA0nQOT3sEQ6Nl\nAQB92N5s+6Dtx7u2/Y3tJ2x/3/ZW22/t895nbP/A9i7bO+qcj0AGUJTMHa691PAlSZcv2LZd0nuS\n/JqkH0n67CLv/3CSi5KsrnMyAhkA+kjykKQXF2y7P8mRNH9Y0pnjOh+BDKAsnU79ZXS/J+nePvsi\n6X7bj9meqfNhXNQDUJQs4aJeFZTdYTmbZLbme/9c0mFJd/U55ENJDth+u6Tttp+oKu6+CGQAy1YV\nvrUCuJvtT0i6UtIl6fPDpEkOVH8P2t4qaY0kAhnAMjLhaW+2L5f0J5J+M8lP+xxzkqTjkrxSrV8m\n6bZBn00gAyhKzdkTtdjeImmtpNNt75d0q+ZnVbxZ820ISXo4yQ223yHpi0nWSVohaWu1/wRJX0ly\n36DzEcgA0EeSDT02b+pz7P9KWlet75P03qWej0AGUJbxzJ6YCgIZQFGWMsuiaZiHDAANQYUMoCxU\nyACAUVEhAyhK5rioBwCN0OaLegQygLK0OJDpIQNAQ1AhAyhKm3vIVMgA0BBUyADK0uIeMoEMoCht\nnmVBywIAGoIKGUBZqJABAKOiQgZQlDZPeyOQAZSlxS0LAhlAUdLiXwyhhwwADTGwQrb9bknrJa2q\nNh2QtC3J3kkODACGUew8ZNt/Kumrkizpv6vFkrbYvnnywwOA5WNQhXy9pAuTvNa90fbtknZL+qtJ\nDQwAhjJXaIUsaU7SO3psX1nt68n2jO0dtnfM/sv2UcYHAEuSTqf20jSDKuSbJD1g+0lJz1bb3inp\nXZI+0+9NSWYlzUqSdt+d0YcJAOVbNJCT3Gf7VySt0esv6j2apHn/vABAAyvfugbOskgyJ+nhYzAW\nABhZsbMsAADHDnfqAShLyS0LAGiTJs6eqIuWBQA0BBUygKK0+fGbVMgA0BAEMoCydDr1lwFsb7Z9\n0PbjXdveZnu77Serv6f2ee911TFP2r6uztAJZABFGfOt01+SdPmCbTdLeiDJeZIeqF6/ju23SbpV\n0vs1f2Pdrf2CuxuBDAB9JHlI0osLNq+XdGe1fqekj/V460clbU/yYpKXJG3XG4P9DbioB6Ao6fR9\n7tm4rEjyXLX+Y0krehyzSkef/yNJ+3X08RN9EcgAyrKEQLY9I2mma9Ns9XC0WpLE9tgeoEYgA1i2\nXvdkyvqet70yyXO2V0o62OOYA5LWdr0+U9KDgz6YHjKAohyD5yFvk3Rk1sR1kr7Z45hvSbrM9qnV\nxbzLqm2LIpABFCWd1F4Gsb1F0nclnW97v+3rNf9LSZdWz4n/req1bK+2/UVJSvKipM9LerRabqu2\nLYqWBQD0kWRDn12X9Dh2h6Tf73q9WdLmpZyPQAZQlGMwy2JiaFkAQENQIQMoSpsrZAIZQFEy197f\nVSaQARSlzuyJpqKHDAANQYUMoChp7/PpqZABoCmokAEUpc09ZAIZQFHm2jvrjZYFADQFFTKAonBR\nDwAwMipkAEVpc4VMIAMoSpsv6hHIAIpChbyIc6+6ZdKnWPae/tfPT3sIxfvIev4/xuRRIQMoytyc\npz2EoTHLAgAaggoZQFG4qAcADdHmi3q0LACgIaiQARSlzRf1CGQARZmjZQEAGBUVMoCi0LIAgIZI\niwOZlgUANAQVMoCitPnGECpkAGgIKmQAReGiHgA0BIEMAA3RaXEg00MGgB5sn297V9dyyPZNC45Z\na/vlrmM+N8o5qZABFGVcLYskP5R0kSTZPl7SAUlbexz6nSRXjuOcVMgAMNglkp5O8j+TPAmBDKAo\nc3HtxfaM7R1dy0yfj71G0pY++z5g+3u277V94Shjp2UBoChLuTEkyayk2cWOsf0mSb8t6bM9du+U\ndHaSV22vk/QNSefVH8HrUSEDwOKukLQzyfMLdyQ5lOTVav0eSSfaPn3YE1EhAyhKJ2Of9rZBfdoV\nts+Q9HyS2F6j+SL3hWFPRCADQB+2T5J0qaQ/7Np2gyQl2Sjpakmfsn1Y0s8kXZMkw56PQAZQlHHe\nqZfk/ySdtmDbxq71OyTdMa7z0UMGgIagQgZQlNfm2ltntnfkAFAYKmQARZnALItjhgoZABqCChlA\nUTpDTzqbPgIZQFHmaFkAAEZFhQygKFzUAwCMbOhAtv3JcQ4EAMahk/pL04xSIf9Fvx3dD30+dOil\nEU4BAEvTkWsvTbNoD9n29/vtkrSi3/u6H/p87i9f0MB/hwCUqomVb12DLuqtkPRRSQvLXEv6r4mM\nCACWqUGB/G+STk6ya+EO2w9OZEQAMILOtAcwgkUDOcn1i+z73fEPBwCWL+YhAyhKmytk5iEDQENQ\nIQMoShOns9VFIAMoSmf43xidOgIZQFHoIQMARkaFDKAoba6QCWQARWlzINOyAICGoEIGUJSO2jvL\nggoZABqCChlAUeghAwBGRoUMoCg/b/GdelTIANAQVMgAisIsCwDAyKiQARRlnBWy7WckvaL5yRuH\nk6xesN+SviBpnaSfSvpEkp3Dno9ABoDFfTjJT/rsu0LSedXyfkn/UP0dCoEMoCjHeB7yeklfThJJ\nD9t+q+2VSZ4b5sPoIQMoSiepvdiesb2ja5lZ8HGRdL/tx3rsk6RVkp7ter2/2jYUKmQAy1aSWUmz\nixzyoSQHbL9d0nbbTyR5aFLjoUIGUJSOUnsZJMmB6u9BSVslrVlwyAFJZ3W9PrPaNhQCGUBRxhXI\ntk+yfcqRdUmXSXp8wWHbJF3reb8h6eVh+8cSLQsAhZkb363TKyRtnZ/ZphMkfSXJfbZvkKQkGyXd\no/kpb09pftrbJ0c5IYEMAD0k2SfpvT22b+xaj6RPj+ucBDKAonDrNABgZFTIAIpChQwAGBkVMoCi\ndFr8gHoCuQDnXnXLtIdQvAcv/oVpDwE10bIAAIyMChlAUcZ4Y8gxRyADKEqbWxYEMoCitDmQ6SED\nQENQIQMoSpt7yFTIANAQVMgAikIPGQAwMipkAEXh1mkAaIi5FrcsCGQARWlzhUwPGQAaggoZQFGY\nhwwAGBkVMoCitHkeMoEMoChzmZv2EIZGywIAGoIKGUBR2jwPmQoZABqCChlAUV6jhwwAGBUVMoCi\ncGMIAGBkVMgAitLeDjIVMgA0BoEMoChzSe1lMbbPsv1t23ts77Z9Y49j1tp+2fauavncKGOnZQGg\nKGO8MeSwpD9OstP2KZIes709yZ4Fx30nyZXjOCEVMgD0kOS5JDur9Vck7ZW0apLnJJABFGVcLYtu\nts+R9D5Jj/TY/QHb37N9r+0LRxk7LQsARVlKy8L2jKSZrk2zSWYXHHOypLsl3ZTk0IKP2Cnp7CSv\n2l4n6RuSzhtq4CKQASxjVfjO9ttv+0TNh/FdSb7e4/2Hutbvsf33tk9P8pNhxkMgAyjKuC7q2bak\nTZL2Jrm9zzFnSHo+SWyv0Xwb+IVhzzkwkG2/W/ON7EeSvNq1/fIk9w17YgCYhLnx3Tn9QUkfl/QD\n27uqbX8m6Z2SlGSjpKslfcr2YUk/k3RNMvy924sGsu0/kvRpzV9d3GT7xiTfrHb/pSQCGUCRkvyH\nJA845g5Jd4zrnIMq5D+Q9OtVw/ocSV+zfU6SL2jAQAFgGtr8gPpBgXzckTZFkmdsr9V8KJ+tRQK5\n+8rl6aedobe85dQxDRcAyjVoHvLzti868qIK5yslnS7pV/u9KclsktVJVhPGAI6lOaX20jSDAvla\nST/u3pDkcJJrJV08sVEBwJCS+kvTLNqySLJ/kX3/Of7hAMBomlj51sWt0wDQENwYAqAo7a2PqZAB\noDGokAEUpc09ZAIZQFHaG8e0LACgMaiQARSFChkAMDIqZABF4aIeADREe+OYQAZQmDYHMj1kAGgI\nKmQARaFCBgCMjAoZQFGokAEAIyOQAaAhaFkAKEzf319uPCpkAGgIKmQAhWlvhUwgAygMgQwAzdDe\nPKaHDABNQYUMoDDtrTPbO3IAKAwVMoCiuMVNZAIZQFnc3kCmZQEADUEgAyiKl/DfwM+yL7f9Q9tP\n2b65x/432/6nav8jts8ZZewEMoDCHLeEpT/bx0v6O0lXSLpA0gbbFyw47HpJLyV5l6S/lfTXo44c\nAPBGayQ9lWRfkp9L+qqk9QuOWS/pzmr9a5IusYdvYhPIAIpiu/YywCpJz3a93l9t63lMksOSXpZ0\n2rBjn/gsi6f37WndJU/bM0lmpz2OkvEdT95y/Y6Xkjm2ZyTNdG2aneZ3RoXc28zgQzAivuPJ4zse\nIMlsktVdS3cYH5B0VtfrM6tt6nWM7RMk/aKkF4YdD4EMAL09Kuk8279k+02SrpG0bcEx2yRdV61f\nLenfkwz9s37cGAIAPSQ5bPszkr4l6XhJm5Pstn2bpB1JtknaJOkfbT8l6UXNh/bQPEKYF2u59t6O\nJb7jyeM7bh8CGQAagh4yADQEgdxl0G2SGJ3tzbYP2n582mMple2zbH/b9h7bu23fOO0xoR5aFpXq\nNskfSbpU8xPAH5W0IcmeqQ6sMLYvlvSqpC8nec+0x1Mi2yslrUyy0/Ypkh6T9DH+X24+KuSj6twm\niREleUjzV6MxIUmeS7KzWn9F0l698Q4zNBCBfFSd2ySBVqmePvY+SY9MdySog0AGCmX7ZEl3S7op\nyaFpjweDEchH1blNEmgF2ydqPozvSvL1aY8H9RDIR9W5TRJovOrxj5sk7U1y+7THg/oI5Er16Lwj\nt0nulfTPSXZPd1Tlsb1F0nclnW97v+3rpz2mAn1Q0sclfcT2rmpZN+1BYTCmvQFAQ1AhA0BDEMgA\n0BAEMgA0BIEMAA1BIANAQxDIANAQBDIANASBDAAN8f8RahFjkt5b0AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSqBnR8JWG65",
        "colab_type": "code",
        "outputId": "0c3762ce-8744-44ac-b329-d21c3eeb3584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      0.81      0.90        16\n",
            "           2       0.75      1.00      0.86         9\n",
            "\n",
            "    accuracy                           0.92        38\n",
            "   macro avg       0.92      0.94      0.92        38\n",
            "weighted avg       0.94      0.92      0.92        38\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up-18g5Knn7I",
        "colab_type": "code",
        "outputId": "aa276d10-a49c-430a-d2b6-0f50ac22bfae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "mlp.coefs_[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.48126151, -0.43699331, -0.34736793, -0.33773779,  0.50410257,\n",
              "         0.48429106,  0.18093484, -0.17601032, -0.3924409 ,  0.28890436,\n",
              "        -0.60650148,  0.15072567, -0.18158802, -0.07550406, -0.16173637,\n",
              "        -0.17810154,  0.26947983, -0.74470642, -0.31022773,  0.1356057 ],\n",
              "       [ 0.0438415 ,  0.09933937,  0.05309406,  0.18705625,  0.3388934 ,\n",
              "         0.21214835,  0.02480627, -0.80130916,  0.21121618,  0.60088148,\n",
              "         0.21586209,  0.26476889, -0.00189491,  0.5422399 , -0.08124901,\n",
              "         0.13622797,  0.54784265, -0.33177176, -0.51977201,  0.07002073],\n",
              "       [ 0.16522159, -0.36487976,  0.44366452,  0.30477944,  0.60088713,\n",
              "         0.48889891,  0.14954842,  0.18637731, -0.44165158,  0.29798026,\n",
              "        -0.41693323,  0.22705155, -0.59974216, -0.47238285,  0.576906  ,\n",
              "        -0.32690239, -0.46094996,  0.46107183,  0.71224007,  0.44863284],\n",
              "       [-0.55616055, -0.10544109,  0.02823422,  0.02955181, -0.15473692,\n",
              "        -0.36010353,  0.68920739, -0.42214404, -0.19892453, -0.00236585,\n",
              "         0.23381493,  0.54670787, -0.05356551, -0.28895049,  0.41114725,\n",
              "         0.05112452,  0.26074859, -0.43772684,  0.25842237,  0.71587999]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF-5HLz8n0vA",
        "colab_type": "code",
        "outputId": "161da054-5d78-41c5-8d50-2bd40e2c4284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "mlp.intercepts_[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.19686612, -0.29664636,  0.65812975, -0.37379071,  0.23689823,\n",
              "       -0.13470439, -0.39195689,  0.02520305, -0.33031081,  0.63408392,\n",
              "       -0.0522206 , -0.06972875, -0.22206588, -0.32518603, -0.29839852,\n",
              "       -0.0944318 , -0.15138707, -0.19507748, -0.09406567,  0.09371023])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0ZzshZYHA6L",
        "colab_type": "text"
      },
      "source": [
        "# Testes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hMvBpgXxbCX",
        "colab_type": "text"
      },
      "source": [
        "## Teste 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To6W4HUBxY3v",
        "colab_type": "code",
        "outputId": "75771a72-32d5-4b90-8f0c-31d1d6a0ef5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=1000, alpha=0.0001, solver='adam', activation='logistic')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Acuracia: {0:.2f}%: \\n\".format(float(accuracy_score(y_test, y_pred)) * 100))\n",
        "print(\"Matriz de Confus達o:\\n\", cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acuracia: 97.37%: \n",
            "\n",
            "Matriz de Confus達o:\n",
            " [[13  0  0]\n",
            " [ 0 15  1]\n",
            " [ 0  0  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guv55saT2XtC",
        "colab_type": "text"
      },
      "source": [
        "## Teste 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4oC2fZYyLn3",
        "colab_type": "code",
        "outputId": "8622f300-644a-4724-d7c2-9e1a67db1424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=1000, solver='adam', activation='relu')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acuracia: 94.74%: \n",
            "\n",
            "Matriz de Confus達o:\n",
            " [[13  0  0]\n",
            " [ 0 15  1]\n",
            " [ 0  0  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sp6DrtZ2d6p",
        "colab_type": "text"
      },
      "source": [
        "## Teste 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5_Ud1Qy2zpV",
        "colab_type": "code",
        "outputId": "72e7d9cd-500d-4107-b46b-79fd19882343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=400, solver='adam', activation='logistic')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Acuracia: {0:.2f}%: \\n\".format(float(accuracy_score(y_test, y_pred)) * 100))\n",
        "print(\"Matriz de Confus達o:\\n\", cm)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acuracia: 81.58%: \n",
            "\n",
            "Matriz de Confus達o:\n",
            " [[13  0  0]\n",
            " [ 0  9  7]\n",
            " [ 0  0  9]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVgv74k-2gvR",
        "colab_type": "text"
      },
      "source": [
        "## Teste 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICRupZHj2_8V",
        "colab_type": "code",
        "outputId": "9333f520-fe52-4593-f3f0-81fb0d6ede0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=1000, solver='adam', activation='tanh')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Acuracia: {0:.2f}%: \\n\".format(float(accuracy_score(y_test, y_pred)) * 100))\n",
        "print(\"Matriz de Confus達o:\\n\", cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acuracia: 94.74%: \n",
            "\n",
            "Matriz de Confus達o:\n",
            " [[13  0  0]\n",
            " [ 0 15  1]\n",
            " [ 0  0  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDjpMj6a2g_j",
        "colab_type": "text"
      },
      "source": [
        "## Teste 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qatNIT7b3iUC",
        "colab_type": "code",
        "outputId": "07af1ebb-cc61-43a2-9deb-e7e04c9aed5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(10,10,10), max_iter=1000, solver='adam', activation='relu')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Acuracia: {0:.2f}%: \\n\".format(float(accuracy_score(y_test, y_pred)) * 100))\n",
        "print(\"Matriz de Confus達o:\\n\", cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acuracia: 97.37%: \n",
            "\n",
            "Matriz de Confus達o:\n",
            " [[13  0  0]\n",
            " [ 0 15  1]\n",
            " [ 0  0  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XlmdBn_2hCL",
        "colab_type": "text"
      },
      "source": [
        "## Teste 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPAGkIc34MAI",
        "colab_type": "code",
        "outputId": "32245cb1-f336-4ded-d1ac-615fc31b35fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(120,120,120), max_iter=1000, solver='sgd', activation='identity')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Acuracia: {0:.2f}%: \\n\".format(float(accuracy_score(y_test, y_pred)) * 100))\n",
        "print(\"Matriz de Confus達o:\\n\", cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acuracia: 97.37%: \n",
            "\n",
            "Matriz de Confus達o:\n",
            " [[13  0  0]\n",
            " [ 0 15  1]\n",
            " [ 0  0  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyhaY1zK2hHn",
        "colab_type": "text"
      },
      "source": [
        "## Teste 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2PYkr8-4wYm",
        "colab_type": "code",
        "outputId": "2995fc91-a2be-41f1-d5a2-970b3db6f798",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=1000, solver='lbfgs', verbose=10, activation='identity')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19kIhS3Z2hLn",
        "colab_type": "text"
      },
      "source": [
        "## Teste 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSPxhshd5Kdb",
        "colab_type": "code",
        "outputId": "c7a2e2da-ce1b-435c-b677-2315036988bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(50,50,50), max_iter=1000, solver='adam', verbose=10, activation='identity')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.90618925\n",
            "Iteration 2, loss = 0.80103430\n",
            "Iteration 3, loss = 0.71299139\n",
            "Iteration 4, loss = 0.64091810\n",
            "Iteration 5, loss = 0.58304710\n",
            "Iteration 6, loss = 0.53717031\n",
            "Iteration 7, loss = 0.50084785\n",
            "Iteration 8, loss = 0.47173922\n",
            "Iteration 9, loss = 0.44788032\n",
            "Iteration 10, loss = 0.42778195\n",
            "Iteration 11, loss = 0.41038915\n",
            "Iteration 12, loss = 0.39499156\n",
            "Iteration 13, loss = 0.38113125\n",
            "Iteration 14, loss = 0.36852186\n",
            "Iteration 15, loss = 0.35698208\n",
            "Iteration 16, loss = 0.34638333\n",
            "Iteration 17, loss = 0.33661150\n",
            "Iteration 18, loss = 0.32754361\n",
            "Iteration 19, loss = 0.31904005\n",
            "Iteration 20, loss = 0.31095114\n",
            "Iteration 21, loss = 0.30313402\n",
            "Iteration 22, loss = 0.29547209\n",
            "Iteration 23, loss = 0.28788897\n",
            "Iteration 24, loss = 0.28035202\n",
            "Iteration 25, loss = 0.27286515\n",
            "Iteration 26, loss = 0.26545463\n",
            "Iteration 27, loss = 0.25815272\n",
            "Iteration 28, loss = 0.25098417\n",
            "Iteration 29, loss = 0.24395858\n",
            "Iteration 30, loss = 0.23706977\n",
            "Iteration 31, loss = 0.23030091\n",
            "Iteration 32, loss = 0.22363282\n",
            "Iteration 33, loss = 0.21705210\n",
            "Iteration 34, loss = 0.21055649\n",
            "Iteration 35, loss = 0.20415588\n",
            "Iteration 36, loss = 0.19786921\n",
            "Iteration 37, loss = 0.19171850\n",
            "Iteration 38, loss = 0.18572246\n",
            "Iteration 39, loss = 0.17989169\n",
            "Iteration 40, loss = 0.17422719\n",
            "Iteration 41, loss = 0.16872253\n",
            "Iteration 42, loss = 0.16336851\n",
            "Iteration 43, loss = 0.15815839\n",
            "Iteration 44, loss = 0.15309139\n",
            "Iteration 45, loss = 0.14817316\n",
            "Iteration 46, loss = 0.14341332\n",
            "Iteration 47, loss = 0.13882152\n",
            "Iteration 48, loss = 0.13440382\n",
            "Iteration 49, loss = 0.13016102\n",
            "Iteration 50, loss = 0.12608932\n",
            "Iteration 51, loss = 0.12218266\n",
            "Iteration 52, loss = 0.11843536\n",
            "Iteration 53, loss = 0.11484386\n",
            "Iteration 54, loss = 0.11140690\n",
            "Iteration 55, loss = 0.10812432\n",
            "Iteration 56, loss = 0.10499528\n",
            "Iteration 57, loss = 0.10201682\n",
            "Iteration 58, loss = 0.09918352\n",
            "Iteration 59, loss = 0.09648818\n",
            "Iteration 60, loss = 0.09392324\n",
            "Iteration 61, loss = 0.09148203\n",
            "Iteration 62, loss = 0.08915943\n",
            "Iteration 63, loss = 0.08695166\n",
            "Iteration 64, loss = 0.08485547\n",
            "Iteration 65, loss = 0.08286725\n",
            "Iteration 66, loss = 0.08098250\n",
            "Iteration 67, loss = 0.07919591\n",
            "Iteration 68, loss = 0.07750181\n",
            "Iteration 69, loss = 0.07589474\n",
            "Iteration 70, loss = 0.07436988\n",
            "Iteration 71, loss = 0.07292299\n",
            "Iteration 72, loss = 0.07155024\n",
            "Iteration 73, loss = 0.07024776\n",
            "Iteration 74, loss = 0.06901141\n",
            "Iteration 75, loss = 0.06783682\n",
            "Iteration 76, loss = 0.06671953\n",
            "Iteration 77, loss = 0.06565536\n",
            "Iteration 78, loss = 0.06464062\n",
            "Iteration 79, loss = 0.06367211\n",
            "Iteration 80, loss = 0.06274708\n",
            "Iteration 81, loss = 0.06186295\n",
            "Iteration 82, loss = 0.06101716\n",
            "Iteration 83, loss = 0.06020718\n",
            "Iteration 84, loss = 0.05943052\n",
            "Iteration 85, loss = 0.05868493\n",
            "Iteration 86, loss = 0.05796840\n",
            "Iteration 87, loss = 0.05727923\n",
            "Iteration 88, loss = 0.05661591\n",
            "Iteration 89, loss = 0.05597702\n",
            "Iteration 90, loss = 0.05536114\n",
            "Iteration 91, loss = 0.05476686\n",
            "Iteration 92, loss = 0.05419280\n",
            "Iteration 93, loss = 0.05363770\n",
            "Iteration 94, loss = 0.05310046\n",
            "Iteration 95, loss = 0.05258013\n",
            "Iteration 96, loss = 0.05207582\n",
            "Iteration 97, loss = 0.05158669\n",
            "Iteration 98, loss = 0.05111191\n",
            "Iteration 99, loss = 0.05065067\n",
            "Iteration 100, loss = 0.05020220\n",
            "Iteration 101, loss = 0.04976582\n",
            "Iteration 102, loss = 0.04934094\n",
            "Iteration 103, loss = 0.04892702\n",
            "Iteration 104, loss = 0.04852355\n",
            "Iteration 105, loss = 0.04813003\n",
            "Iteration 106, loss = 0.04774598\n",
            "Iteration 107, loss = 0.04737091\n",
            "Iteration 108, loss = 0.04700440\n",
            "Iteration 109, loss = 0.04664606\n",
            "Iteration 110, loss = 0.04629554\n",
            "Iteration 111, loss = 0.04595250\n",
            "Iteration 112, loss = 0.04561661\n",
            "Iteration 113, loss = 0.04528756\n",
            "Iteration 114, loss = 0.04496503\n",
            "Iteration 115, loss = 0.04464874\n",
            "Iteration 116, loss = 0.04433843\n",
            "Iteration 117, loss = 0.04403386\n",
            "Iteration 118, loss = 0.04373480\n",
            "Iteration 119, loss = 0.04344104\n",
            "Iteration 120, loss = 0.04315238\n",
            "Iteration 121, loss = 0.04286860\n",
            "Iteration 122, loss = 0.04258952\n",
            "Iteration 123, loss = 0.04231498\n",
            "Iteration 124, loss = 0.04204483\n",
            "Iteration 125, loss = 0.04177892\n",
            "Iteration 126, loss = 0.04151711\n",
            "Iteration 127, loss = 0.04125927\n",
            "Iteration 128, loss = 0.04100527\n",
            "Iteration 129, loss = 0.04075499\n",
            "Iteration 130, loss = 0.04050833\n",
            "Iteration 131, loss = 0.04026519\n",
            "Iteration 132, loss = 0.04002546\n",
            "Iteration 133, loss = 0.03978905\n",
            "Iteration 134, loss = 0.03955588\n",
            "Iteration 135, loss = 0.03932584\n",
            "Iteration 136, loss = 0.03909887\n",
            "Iteration 137, loss = 0.03887488\n",
            "Iteration 138, loss = 0.03865380\n",
            "Iteration 139, loss = 0.03843557\n",
            "Iteration 140, loss = 0.03822011\n",
            "Iteration 141, loss = 0.03800736\n",
            "Iteration 142, loss = 0.03779726\n",
            "Iteration 143, loss = 0.03758975\n",
            "Iteration 144, loss = 0.03738477\n",
            "Iteration 145, loss = 0.03718227\n",
            "Iteration 146, loss = 0.03698220\n",
            "Iteration 147, loss = 0.03678451\n",
            "Iteration 148, loss = 0.03658916\n",
            "Iteration 149, loss = 0.03639608\n",
            "Iteration 150, loss = 0.03620525\n",
            "Iteration 151, loss = 0.03601662\n",
            "Iteration 152, loss = 0.03583016\n",
            "Iteration 153, loss = 0.03564581\n",
            "Iteration 154, loss = 0.03546355\n",
            "Iteration 155, loss = 0.03528333\n",
            "Iteration 156, loss = 0.03510513\n",
            "Iteration 157, loss = 0.03492891\n",
            "Iteration 158, loss = 0.03475464\n",
            "Iteration 159, loss = 0.03458228\n",
            "Iteration 160, loss = 0.03441181\n",
            "Iteration 161, loss = 0.03424319\n",
            "Iteration 162, loss = 0.03407640\n",
            "Iteration 163, loss = 0.03391140\n",
            "Iteration 164, loss = 0.03374818\n",
            "Iteration 165, loss = 0.03358671\n",
            "Iteration 166, loss = 0.03342695\n",
            "Iteration 167, loss = 0.03326888\n",
            "Iteration 168, loss = 0.03311249\n",
            "Iteration 169, loss = 0.03295774\n",
            "Iteration 170, loss = 0.03280461\n",
            "Iteration 171, loss = 0.03265308\n",
            "Iteration 172, loss = 0.03250313\n",
            "Iteration 173, loss = 0.03235473\n",
            "Iteration 174, loss = 0.03220787\n",
            "Iteration 175, loss = 0.03206251\n",
            "Iteration 176, loss = 0.03191865\n",
            "Iteration 177, loss = 0.03177627\n",
            "Iteration 178, loss = 0.03163533\n",
            "Iteration 179, loss = 0.03149582\n",
            "Iteration 180, loss = 0.03135773\n",
            "Iteration 181, loss = 0.03122104\n",
            "Iteration 182, loss = 0.03108572\n",
            "Iteration 183, loss = 0.03095176\n",
            "Iteration 184, loss = 0.03081913\n",
            "Iteration 185, loss = 0.03068783\n",
            "Iteration 186, loss = 0.03055784\n",
            "Iteration 187, loss = 0.03042914\n",
            "Iteration 188, loss = 0.03030170\n",
            "Iteration 189, loss = 0.03017552\n",
            "Iteration 190, loss = 0.03005058\n",
            "Iteration 191, loss = 0.02992687\n",
            "Iteration 192, loss = 0.02980436\n",
            "Iteration 193, loss = 0.02968304\n",
            "Iteration 194, loss = 0.02956290\n",
            "Iteration 195, loss = 0.02944392\n",
            "Iteration 196, loss = 0.02932608\n",
            "Iteration 197, loss = 0.02920938\n",
            "Iteration 198, loss = 0.02909379\n",
            "Iteration 199, loss = 0.02897931\n",
            "Iteration 200, loss = 0.02886591\n",
            "Iteration 201, loss = 0.02875358\n",
            "Iteration 202, loss = 0.02864232\n",
            "Iteration 203, loss = 0.02853210\n",
            "Iteration 204, loss = 0.02842292\n",
            "Iteration 205, loss = 0.02831475\n",
            "Iteration 206, loss = 0.02820759\n",
            "Iteration 207, loss = 0.02810142\n",
            "Iteration 208, loss = 0.02799623\n",
            "Iteration 209, loss = 0.02789201\n",
            "Iteration 210, loss = 0.02778874\n",
            "Iteration 211, loss = 0.02768641\n",
            "Iteration 212, loss = 0.02758501\n",
            "Iteration 213, loss = 0.02748453\n",
            "Iteration 214, loss = 0.02738495\n",
            "Iteration 215, loss = 0.02728627\n",
            "Iteration 216, loss = 0.02718846\n",
            "Iteration 217, loss = 0.02709153\n",
            "Iteration 218, loss = 0.02699545\n",
            "Iteration 219, loss = 0.02690022\n",
            "Iteration 220, loss = 0.02680582\n",
            "Iteration 221, loss = 0.02671224\n",
            "Iteration 222, loss = 0.02661948\n",
            "Iteration 223, loss = 0.02652752\n",
            "Iteration 224, loss = 0.02643635\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ttzrGER2qof",
        "colab_type": "text"
      },
      "source": [
        "## Teste 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTib3Aux5p9f",
        "colab_type": "code",
        "outputId": "f44b0e37-729e-473e-b9a8-3429616a9174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=1000, solver='adam', verbose=10, activation='relu', learning_rate='adaptive')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.01287815\n",
            "Iteration 2, loss = 0.99802354\n",
            "Iteration 3, loss = 0.98341442\n",
            "Iteration 4, loss = 0.96900242\n",
            "Iteration 5, loss = 0.95474443\n",
            "Iteration 6, loss = 0.94062553\n",
            "Iteration 7, loss = 0.92666526\n",
            "Iteration 8, loss = 0.91286336\n",
            "Iteration 9, loss = 0.89926335\n",
            "Iteration 10, loss = 0.88583446\n",
            "Iteration 11, loss = 0.87254442\n",
            "Iteration 12, loss = 0.85939676\n",
            "Iteration 13, loss = 0.84646754\n",
            "Iteration 14, loss = 0.83368723\n",
            "Iteration 15, loss = 0.82108252\n",
            "Iteration 16, loss = 0.80868480\n",
            "Iteration 17, loss = 0.79651358\n",
            "Iteration 18, loss = 0.78458672\n",
            "Iteration 19, loss = 0.77290923\n",
            "Iteration 20, loss = 0.76149633\n",
            "Iteration 21, loss = 0.75034278\n",
            "Iteration 22, loss = 0.73946390\n",
            "Iteration 23, loss = 0.72885240\n",
            "Iteration 24, loss = 0.71851915\n",
            "Iteration 25, loss = 0.70845259\n",
            "Iteration 26, loss = 0.69866351\n",
            "Iteration 27, loss = 0.68915905\n",
            "Iteration 28, loss = 0.67989879\n",
            "Iteration 29, loss = 0.67091269\n",
            "Iteration 30, loss = 0.66218048\n",
            "Iteration 31, loss = 0.65370438\n",
            "Iteration 32, loss = 0.64549707\n",
            "Iteration 33, loss = 0.63752861\n",
            "Iteration 34, loss = 0.62979679\n",
            "Iteration 35, loss = 0.62229799\n",
            "Iteration 36, loss = 0.61499697\n",
            "Iteration 37, loss = 0.60787993\n",
            "Iteration 38, loss = 0.60093298\n",
            "Iteration 39, loss = 0.59415318\n",
            "Iteration 40, loss = 0.58755667\n",
            "Iteration 41, loss = 0.58113978\n",
            "Iteration 42, loss = 0.57484744\n",
            "Iteration 43, loss = 0.56867667\n",
            "Iteration 44, loss = 0.56263792\n",
            "Iteration 45, loss = 0.55665333\n",
            "Iteration 46, loss = 0.55073115\n",
            "Iteration 47, loss = 0.54488348\n",
            "Iteration 48, loss = 0.53913138\n",
            "Iteration 49, loss = 0.53344937\n",
            "Iteration 50, loss = 0.52781689\n",
            "Iteration 51, loss = 0.52224667\n",
            "Iteration 52, loss = 0.51676122\n",
            "Iteration 53, loss = 0.51133604\n",
            "Iteration 54, loss = 0.50596142\n",
            "Iteration 55, loss = 0.50063712\n",
            "Iteration 56, loss = 0.49533802\n",
            "Iteration 57, loss = 0.49005750\n",
            "Iteration 58, loss = 0.48482091\n",
            "Iteration 59, loss = 0.47961906\n",
            "Iteration 60, loss = 0.47441535\n",
            "Iteration 61, loss = 0.46920058\n",
            "Iteration 62, loss = 0.46402944\n",
            "Iteration 63, loss = 0.45891027\n",
            "Iteration 64, loss = 0.45384795\n",
            "Iteration 65, loss = 0.44881442\n",
            "Iteration 66, loss = 0.44381269\n",
            "Iteration 67, loss = 0.43882315\n",
            "Iteration 68, loss = 0.43387723\n",
            "Iteration 69, loss = 0.42895310\n",
            "Iteration 70, loss = 0.42402208\n",
            "Iteration 71, loss = 0.41913114\n",
            "Iteration 72, loss = 0.41422416\n",
            "Iteration 73, loss = 0.40931697\n",
            "Iteration 74, loss = 0.40438291\n",
            "Iteration 75, loss = 0.39944334\n",
            "Iteration 76, loss = 0.39448144\n",
            "Iteration 77, loss = 0.38949565\n",
            "Iteration 78, loss = 0.38452402\n",
            "Iteration 79, loss = 0.37957666\n",
            "Iteration 80, loss = 0.37463044\n",
            "Iteration 81, loss = 0.36967292\n",
            "Iteration 82, loss = 0.36470803\n",
            "Iteration 83, loss = 0.35977583\n",
            "Iteration 84, loss = 0.35489943\n",
            "Iteration 85, loss = 0.35005782\n",
            "Iteration 86, loss = 0.34524181\n",
            "Iteration 87, loss = 0.34044326\n",
            "Iteration 88, loss = 0.33563329\n",
            "Iteration 89, loss = 0.33081509\n",
            "Iteration 90, loss = 0.32602039\n",
            "Iteration 91, loss = 0.32126279\n",
            "Iteration 92, loss = 0.31655328\n",
            "Iteration 93, loss = 0.31187580\n",
            "Iteration 94, loss = 0.30722433\n",
            "Iteration 95, loss = 0.30263153\n",
            "Iteration 96, loss = 0.29808761\n",
            "Iteration 97, loss = 0.29357692\n",
            "Iteration 98, loss = 0.28908803\n",
            "Iteration 99, loss = 0.28461391\n",
            "Iteration 100, loss = 0.28017496\n",
            "Iteration 101, loss = 0.27578447\n",
            "Iteration 102, loss = 0.27142734\n",
            "Iteration 103, loss = 0.26711626\n",
            "Iteration 104, loss = 0.26285727\n",
            "Iteration 105, loss = 0.25863823\n",
            "Iteration 106, loss = 0.25444579\n",
            "Iteration 107, loss = 0.25028922\n",
            "Iteration 108, loss = 0.24618740\n",
            "Iteration 109, loss = 0.24213534\n",
            "Iteration 110, loss = 0.23812386\n",
            "Iteration 111, loss = 0.23416566\n",
            "Iteration 112, loss = 0.23027497\n",
            "Iteration 113, loss = 0.22645661\n",
            "Iteration 114, loss = 0.22270009\n",
            "Iteration 115, loss = 0.21900084\n",
            "Iteration 116, loss = 0.21534086\n",
            "Iteration 117, loss = 0.21173734\n",
            "Iteration 118, loss = 0.20819671\n",
            "Iteration 119, loss = 0.20473597\n",
            "Iteration 120, loss = 0.20134028\n",
            "Iteration 121, loss = 0.19799961\n",
            "Iteration 122, loss = 0.19470571\n",
            "Iteration 123, loss = 0.19146814\n",
            "Iteration 124, loss = 0.18826688\n",
            "Iteration 125, loss = 0.18509363\n",
            "Iteration 126, loss = 0.18196944\n",
            "Iteration 127, loss = 0.17887511\n",
            "Iteration 128, loss = 0.17581840\n",
            "Iteration 129, loss = 0.17279541\n",
            "Iteration 130, loss = 0.16981873\n",
            "Iteration 131, loss = 0.16688329\n",
            "Iteration 132, loss = 0.16399782\n",
            "Iteration 133, loss = 0.16116807\n",
            "Iteration 134, loss = 0.15838749\n",
            "Iteration 135, loss = 0.15564646\n",
            "Iteration 136, loss = 0.15294919\n",
            "Iteration 137, loss = 0.15030023\n",
            "Iteration 138, loss = 0.14770121\n",
            "Iteration 139, loss = 0.14515358\n",
            "Iteration 140, loss = 0.14261619\n",
            "Iteration 141, loss = 0.14011910\n",
            "Iteration 142, loss = 0.13765115\n",
            "Iteration 143, loss = 0.13523908\n",
            "Iteration 144, loss = 0.13289001\n",
            "Iteration 145, loss = 0.13060224\n",
            "Iteration 146, loss = 0.12837037\n",
            "Iteration 147, loss = 0.12616879\n",
            "Iteration 148, loss = 0.12402522\n",
            "Iteration 149, loss = 0.12194088\n",
            "Iteration 150, loss = 0.11991676\n",
            "Iteration 151, loss = 0.11795557\n",
            "Iteration 152, loss = 0.11606220\n",
            "Iteration 153, loss = 0.11424514\n",
            "Iteration 154, loss = 0.11248326\n",
            "Iteration 155, loss = 0.11077332\n",
            "Iteration 156, loss = 0.10911910\n",
            "Iteration 157, loss = 0.10751657\n",
            "Iteration 158, loss = 0.10595535\n",
            "Iteration 159, loss = 0.10444521\n",
            "Iteration 160, loss = 0.10297155\n",
            "Iteration 161, loss = 0.10153665\n",
            "Iteration 162, loss = 0.10013971\n",
            "Iteration 163, loss = 0.09877576\n",
            "Iteration 164, loss = 0.09744508\n",
            "Iteration 165, loss = 0.09614734\n",
            "Iteration 166, loss = 0.09488329\n",
            "Iteration 167, loss = 0.09365265\n",
            "Iteration 168, loss = 0.09245164\n",
            "Iteration 169, loss = 0.09128172\n",
            "Iteration 170, loss = 0.09014553\n",
            "Iteration 171, loss = 0.08903723\n",
            "Iteration 172, loss = 0.08795640\n",
            "Iteration 173, loss = 0.08690202\n",
            "Iteration 174, loss = 0.08587377\n",
            "Iteration 175, loss = 0.08487139\n",
            "Iteration 176, loss = 0.08389399\n",
            "Iteration 177, loss = 0.08294086\n",
            "Iteration 178, loss = 0.08201031\n",
            "Iteration 179, loss = 0.08110506\n",
            "Iteration 180, loss = 0.08022324\n",
            "Iteration 181, loss = 0.07938623\n",
            "Iteration 182, loss = 0.07857771\n",
            "Iteration 183, loss = 0.07778631\n",
            "Iteration 184, loss = 0.07701024\n",
            "Iteration 185, loss = 0.07624985\n",
            "Iteration 186, loss = 0.07550797\n",
            "Iteration 187, loss = 0.07478228\n",
            "Iteration 188, loss = 0.07407197\n",
            "Iteration 189, loss = 0.07337531\n",
            "Iteration 190, loss = 0.07269494\n",
            "Iteration 191, loss = 0.07202893\n",
            "Iteration 192, loss = 0.07137761\n",
            "Iteration 193, loss = 0.07074095\n",
            "Iteration 194, loss = 0.07011808\n",
            "Iteration 195, loss = 0.06950841\n",
            "Iteration 196, loss = 0.06891106\n",
            "Iteration 197, loss = 0.06832927\n",
            "Iteration 198, loss = 0.06776873\n",
            "Iteration 199, loss = 0.06721729\n",
            "Iteration 200, loss = 0.06667338\n",
            "Iteration 201, loss = 0.06613842\n",
            "Iteration 202, loss = 0.06561235\n",
            "Iteration 203, loss = 0.06509558\n",
            "Iteration 204, loss = 0.06458737\n",
            "Iteration 205, loss = 0.06409724\n",
            "Iteration 206, loss = 0.06361776\n",
            "Iteration 207, loss = 0.06314497\n",
            "Iteration 208, loss = 0.06267908\n",
            "Iteration 209, loss = 0.06221949\n",
            "Iteration 210, loss = 0.06176655\n",
            "Iteration 211, loss = 0.06131997\n",
            "Iteration 212, loss = 0.06088406\n",
            "Iteration 213, loss = 0.06046092\n",
            "Iteration 214, loss = 0.06004107\n",
            "Iteration 215, loss = 0.05962537\n",
            "Iteration 216, loss = 0.05921685\n",
            "Iteration 217, loss = 0.05881563\n",
            "Iteration 218, loss = 0.05842302\n",
            "Iteration 219, loss = 0.05803362\n",
            "Iteration 220, loss = 0.05764742\n",
            "Iteration 221, loss = 0.05727232\n",
            "Iteration 222, loss = 0.05690597\n",
            "Iteration 223, loss = 0.05654078\n",
            "Iteration 224, loss = 0.05618138\n",
            "Iteration 225, loss = 0.05582958\n",
            "Iteration 226, loss = 0.05548009\n",
            "Iteration 227, loss = 0.05514094\n",
            "Iteration 228, loss = 0.05480504\n",
            "Iteration 229, loss = 0.05447010\n",
            "Iteration 230, loss = 0.05412676\n",
            "Iteration 231, loss = 0.05378317\n",
            "Iteration 232, loss = 0.05343346\n",
            "Iteration 233, loss = 0.05308827\n",
            "Iteration 234, loss = 0.05274069\n",
            "Iteration 235, loss = 0.05238965\n",
            "Iteration 236, loss = 0.05204438\n",
            "Iteration 237, loss = 0.05170119\n",
            "Iteration 238, loss = 0.05135606\n",
            "Iteration 239, loss = 0.05100890\n",
            "Iteration 240, loss = 0.05066294\n",
            "Iteration 241, loss = 0.05031634\n",
            "Iteration 242, loss = 0.04996850\n",
            "Iteration 243, loss = 0.04962280\n",
            "Iteration 244, loss = 0.04927674\n",
            "Iteration 245, loss = 0.04894593\n",
            "Iteration 246, loss = 0.04864236\n",
            "Iteration 247, loss = 0.04835151\n",
            "Iteration 248, loss = 0.04806372\n",
            "Iteration 249, loss = 0.04777660\n",
            "Iteration 250, loss = 0.04748899\n",
            "Iteration 251, loss = 0.04720766\n",
            "Iteration 252, loss = 0.04692786\n",
            "Iteration 253, loss = 0.04665180\n",
            "Iteration 254, loss = 0.04637625\n",
            "Iteration 255, loss = 0.04610925\n",
            "Iteration 256, loss = 0.04583978\n",
            "Iteration 257, loss = 0.04556823\n",
            "Iteration 258, loss = 0.04530590\n",
            "Iteration 259, loss = 0.04504229\n",
            "Iteration 260, loss = 0.04477825\n",
            "Iteration 261, loss = 0.04451734\n",
            "Iteration 262, loss = 0.04425370\n",
            "Iteration 263, loss = 0.04399755\n",
            "Iteration 264, loss = 0.04374974\n",
            "Iteration 265, loss = 0.04349827\n",
            "Iteration 266, loss = 0.04324268\n",
            "Iteration 267, loss = 0.04298320\n",
            "Iteration 268, loss = 0.04274227\n",
            "Iteration 269, loss = 0.04250299\n",
            "Iteration 270, loss = 0.04225875\n",
            "Iteration 271, loss = 0.04201029\n",
            "Iteration 272, loss = 0.04177036\n",
            "Iteration 273, loss = 0.04152761\n",
            "Iteration 274, loss = 0.04129215\n",
            "Iteration 275, loss = 0.04105562\n",
            "Iteration 276, loss = 0.04081913\n",
            "Iteration 277, loss = 0.04058379\n",
            "Iteration 278, loss = 0.04034812\n",
            "Iteration 279, loss = 0.04011676\n",
            "Iteration 280, loss = 0.03988535\n",
            "Iteration 281, loss = 0.03965805\n",
            "Iteration 282, loss = 0.03943049\n",
            "Iteration 283, loss = 0.03919886\n",
            "Iteration 284, loss = 0.03897509\n",
            "Iteration 285, loss = 0.03875301\n",
            "Iteration 286, loss = 0.03853763\n",
            "Iteration 287, loss = 0.03831972\n",
            "Iteration 288, loss = 0.03809716\n",
            "Iteration 289, loss = 0.03788410\n",
            "Iteration 290, loss = 0.03767986\n",
            "Iteration 291, loss = 0.03747213\n",
            "Iteration 292, loss = 0.03726799\n",
            "Iteration 293, loss = 0.03706020\n",
            "Iteration 294, loss = 0.03684943\n",
            "Iteration 295, loss = 0.03665313\n",
            "Iteration 296, loss = 0.03645320\n",
            "Iteration 297, loss = 0.03624936\n",
            "Iteration 298, loss = 0.03604419\n",
            "Iteration 299, loss = 0.03583782\n",
            "Iteration 300, loss = 0.03563471\n",
            "Iteration 301, loss = 0.03543697\n",
            "Iteration 302, loss = 0.03524380\n",
            "Iteration 303, loss = 0.03505052\n",
            "Iteration 304, loss = 0.03485274\n",
            "Iteration 305, loss = 0.03465374\n",
            "Iteration 306, loss = 0.03445411\n",
            "Iteration 307, loss = 0.03426111\n",
            "Iteration 308, loss = 0.03407132\n",
            "Iteration 309, loss = 0.03388440\n",
            "Iteration 310, loss = 0.03369390\n",
            "Iteration 311, loss = 0.03351775\n",
            "Iteration 312, loss = 0.03333584\n",
            "Iteration 313, loss = 0.03314356\n",
            "Iteration 314, loss = 0.03297900\n",
            "Iteration 315, loss = 0.03280480\n",
            "Iteration 316, loss = 0.03262286\n",
            "Iteration 317, loss = 0.03245040\n",
            "Iteration 318, loss = 0.03228399\n",
            "Iteration 319, loss = 0.03211032\n",
            "Iteration 320, loss = 0.03193979\n",
            "Iteration 321, loss = 0.03176553\n",
            "Iteration 322, loss = 0.03160255\n",
            "Iteration 323, loss = 0.03143666\n",
            "Iteration 324, loss = 0.03125947\n",
            "Iteration 325, loss = 0.03110014\n",
            "Iteration 326, loss = 0.03094776\n",
            "Iteration 327, loss = 0.03077651\n",
            "Iteration 328, loss = 0.03060106\n",
            "Iteration 329, loss = 0.03045671\n",
            "Iteration 330, loss = 0.03030305\n",
            "Iteration 331, loss = 0.03013998\n",
            "Iteration 332, loss = 0.02996863\n",
            "Iteration 333, loss = 0.02980923\n",
            "Iteration 334, loss = 0.02966297\n",
            "Iteration 335, loss = 0.02950068\n",
            "Iteration 336, loss = 0.02933977\n",
            "Iteration 337, loss = 0.02918851\n",
            "Iteration 338, loss = 0.02902917\n",
            "Iteration 339, loss = 0.02887451\n",
            "Iteration 340, loss = 0.02872923\n",
            "Iteration 341, loss = 0.02857692\n",
            "Iteration 342, loss = 0.02841471\n",
            "Iteration 343, loss = 0.02828220\n",
            "Iteration 344, loss = 0.02813971\n",
            "Iteration 345, loss = 0.02798776\n",
            "Iteration 346, loss = 0.02782426\n",
            "Iteration 347, loss = 0.02768044\n",
            "Iteration 348, loss = 0.02753318\n",
            "Iteration 349, loss = 0.02738603\n",
            "Iteration 350, loss = 0.02723831\n",
            "Iteration 351, loss = 0.02710093\n",
            "Iteration 352, loss = 0.02695420\n",
            "Iteration 353, loss = 0.02680033\n",
            "Iteration 354, loss = 0.02665975\n",
            "Iteration 355, loss = 0.02652384\n",
            "Iteration 356, loss = 0.02638203\n",
            "Iteration 357, loss = 0.02624380\n",
            "Iteration 358, loss = 0.02610225\n",
            "Iteration 359, loss = 0.02596323\n",
            "Iteration 360, loss = 0.02582523\n",
            "Iteration 361, loss = 0.02568628\n",
            "Iteration 362, loss = 0.02554730\n",
            "Iteration 363, loss = 0.02541327\n",
            "Iteration 364, loss = 0.02527718\n",
            "Iteration 365, loss = 0.02514007\n",
            "Iteration 366, loss = 0.02500667\n",
            "Iteration 367, loss = 0.02487660\n",
            "Iteration 368, loss = 0.02474092\n",
            "Iteration 369, loss = 0.02460962\n",
            "Iteration 370, loss = 0.02447757\n",
            "Iteration 371, loss = 0.02434613\n",
            "Iteration 372, loss = 0.02421919\n",
            "Iteration 373, loss = 0.02408328\n",
            "Iteration 374, loss = 0.02395355\n",
            "Iteration 375, loss = 0.02382382\n",
            "Iteration 376, loss = 0.02370992\n",
            "Iteration 377, loss = 0.02358259\n",
            "Iteration 378, loss = 0.02344444\n",
            "Iteration 379, loss = 0.02331613\n",
            "Iteration 380, loss = 0.02320206\n",
            "Iteration 381, loss = 0.02307590\n",
            "Iteration 382, loss = 0.02295379\n",
            "Iteration 383, loss = 0.02282737\n",
            "Iteration 384, loss = 0.02269301\n",
            "Iteration 385, loss = 0.02257976\n",
            "Iteration 386, loss = 0.02245600\n",
            "Iteration 387, loss = 0.02233811\n",
            "Iteration 388, loss = 0.02221345\n",
            "Iteration 389, loss = 0.02207895\n",
            "Iteration 390, loss = 0.02196961\n",
            "Iteration 391, loss = 0.02185688\n",
            "Iteration 392, loss = 0.02173117\n",
            "Iteration 393, loss = 0.02162366\n",
            "Iteration 394, loss = 0.02150625\n",
            "Iteration 395, loss = 0.02137967\n",
            "Iteration 396, loss = 0.02125627\n",
            "Iteration 397, loss = 0.02114712\n",
            "Iteration 398, loss = 0.02102221\n",
            "Iteration 399, loss = 0.02090868\n",
            "Iteration 400, loss = 0.02079683\n",
            "Iteration 401, loss = 0.02067526\n",
            "Iteration 402, loss = 0.02055949\n",
            "Iteration 403, loss = 0.02045162\n",
            "Iteration 404, loss = 0.02033762\n",
            "Iteration 405, loss = 0.02022696\n",
            "Iteration 406, loss = 0.02011264\n",
            "Iteration 407, loss = 0.01999188\n",
            "Iteration 408, loss = 0.01988866\n",
            "Iteration 409, loss = 0.01977252\n",
            "Iteration 410, loss = 0.01967519\n",
            "Iteration 411, loss = 0.01956874\n",
            "Iteration 412, loss = 0.01944789\n",
            "Iteration 413, loss = 0.01934423\n",
            "Iteration 414, loss = 0.01924137\n",
            "Iteration 415, loss = 0.01912399\n",
            "Iteration 416, loss = 0.01901836\n",
            "Iteration 417, loss = 0.01892121\n",
            "Iteration 418, loss = 0.01881252\n",
            "Iteration 419, loss = 0.01870058\n",
            "Iteration 420, loss = 0.01859574\n",
            "Iteration 421, loss = 0.01847844\n",
            "Iteration 422, loss = 0.01837556\n",
            "Iteration 423, loss = 0.01827580\n",
            "Iteration 424, loss = 0.01817287\n",
            "Iteration 425, loss = 0.01807337\n",
            "Iteration 426, loss = 0.01796326\n",
            "Iteration 427, loss = 0.01786424\n",
            "Iteration 428, loss = 0.01776020\n",
            "Iteration 429, loss = 0.01765128\n",
            "Iteration 430, loss = 0.01756325\n",
            "Iteration 431, loss = 0.01746291\n",
            "Iteration 432, loss = 0.01735192\n",
            "Iteration 433, loss = 0.01724926\n",
            "Iteration 434, loss = 0.01715320\n",
            "Iteration 435, loss = 0.01704856\n",
            "Iteration 436, loss = 0.01694801\n",
            "Iteration 437, loss = 0.01684872\n",
            "Iteration 438, loss = 0.01674237\n",
            "Iteration 439, loss = 0.01665739\n",
            "Iteration 440, loss = 0.01656232\n",
            "Iteration 441, loss = 0.01645890\n",
            "Iteration 442, loss = 0.01634975\n",
            "Iteration 443, loss = 0.01627242\n",
            "Iteration 444, loss = 0.01618479\n",
            "Iteration 445, loss = 0.01608325\n",
            "Iteration 446, loss = 0.01597067\n",
            "Iteration 447, loss = 0.01587913\n",
            "Iteration 448, loss = 0.01579243\n",
            "Iteration 449, loss = 0.01569373\n",
            "Iteration 450, loss = 0.01559508\n",
            "Iteration 451, loss = 0.01549702\n",
            "Iteration 452, loss = 0.01540972\n",
            "Iteration 453, loss = 0.01531547\n",
            "Iteration 454, loss = 0.01522615\n",
            "Iteration 455, loss = 0.01513701\n",
            "Iteration 456, loss = 0.01504209\n",
            "Iteration 457, loss = 0.01495548\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He-QjpuH2rG_",
        "colab_type": "text"
      },
      "source": [
        "## Teste 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klr28-IR6O8S",
        "colab_type": "code",
        "outputId": "7e75d88b-fbf6-41b1-a485-09f4df639666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(20,20,20), max_iter=1000, solver='adam', verbose=10, activation='relu', learning_rate='constant')\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.28635303\n",
            "Iteration 2, loss = 1.26571706\n",
            "Iteration 3, loss = 1.24573770\n",
            "Iteration 4, loss = 1.22639573\n",
            "Iteration 5, loss = 1.20776488\n",
            "Iteration 6, loss = 1.18988199\n",
            "Iteration 7, loss = 1.17265336\n",
            "Iteration 8, loss = 1.15615158\n",
            "Iteration 9, loss = 1.14030757\n",
            "Iteration 10, loss = 1.12506525\n",
            "Iteration 11, loss = 1.11042173\n",
            "Iteration 12, loss = 1.09630973\n",
            "Iteration 13, loss = 1.08266565\n",
            "Iteration 14, loss = 1.06958211\n",
            "Iteration 15, loss = 1.05686264\n",
            "Iteration 16, loss = 1.04456439\n",
            "Iteration 17, loss = 1.03259435\n",
            "Iteration 18, loss = 1.02095801\n",
            "Iteration 19, loss = 1.00964379\n",
            "Iteration 20, loss = 0.99869307\n",
            "Iteration 21, loss = 0.98802477\n",
            "Iteration 22, loss = 0.97753345\n",
            "Iteration 23, loss = 0.96720196\n",
            "Iteration 24, loss = 0.95701549\n",
            "Iteration 25, loss = 0.94697622\n",
            "Iteration 26, loss = 0.93712330\n",
            "Iteration 27, loss = 0.92745227\n",
            "Iteration 28, loss = 0.91789105\n",
            "Iteration 29, loss = 0.90846457\n",
            "Iteration 30, loss = 0.89917728\n",
            "Iteration 31, loss = 0.89003273\n",
            "Iteration 32, loss = 0.88098690\n",
            "Iteration 33, loss = 0.87208104\n",
            "Iteration 34, loss = 0.86327220\n",
            "Iteration 35, loss = 0.85456324\n",
            "Iteration 36, loss = 0.84592251\n",
            "Iteration 37, loss = 0.83750459\n",
            "Iteration 38, loss = 0.82917611\n",
            "Iteration 39, loss = 0.82090355\n",
            "Iteration 40, loss = 0.81270041\n",
            "Iteration 41, loss = 0.80459036\n",
            "Iteration 42, loss = 0.79659883\n",
            "Iteration 43, loss = 0.78871310\n",
            "Iteration 44, loss = 0.78089031\n",
            "Iteration 45, loss = 0.77318395\n",
            "Iteration 46, loss = 0.76556492\n",
            "Iteration 47, loss = 0.75799016\n",
            "Iteration 48, loss = 0.75047383\n",
            "Iteration 49, loss = 0.74299103\n",
            "Iteration 50, loss = 0.73554463\n",
            "Iteration 51, loss = 0.72821307\n",
            "Iteration 52, loss = 0.72089608\n",
            "Iteration 53, loss = 0.71360636\n",
            "Iteration 54, loss = 0.70637003\n",
            "Iteration 55, loss = 0.69919379\n",
            "Iteration 56, loss = 0.69208508\n",
            "Iteration 57, loss = 0.68502672\n",
            "Iteration 58, loss = 0.67798488\n",
            "Iteration 59, loss = 0.67096287\n",
            "Iteration 60, loss = 0.66394814\n",
            "Iteration 61, loss = 0.65695123\n",
            "Iteration 62, loss = 0.65000062\n",
            "Iteration 63, loss = 0.64309936\n",
            "Iteration 64, loss = 0.63620287\n",
            "Iteration 65, loss = 0.62930167\n",
            "Iteration 66, loss = 0.62242374\n",
            "Iteration 67, loss = 0.61559236\n",
            "Iteration 68, loss = 0.60878305\n",
            "Iteration 69, loss = 0.60200892\n",
            "Iteration 70, loss = 0.59528877\n",
            "Iteration 71, loss = 0.58857407\n",
            "Iteration 72, loss = 0.58185001\n",
            "Iteration 73, loss = 0.57514708\n",
            "Iteration 74, loss = 0.56844411\n",
            "Iteration 75, loss = 0.56174588\n",
            "Iteration 76, loss = 0.55505516\n",
            "Iteration 77, loss = 0.54832134\n",
            "Iteration 78, loss = 0.54158197\n",
            "Iteration 79, loss = 0.53488474\n",
            "Iteration 80, loss = 0.52820889\n",
            "Iteration 81, loss = 0.52157661\n",
            "Iteration 82, loss = 0.51497138\n",
            "Iteration 83, loss = 0.50838151\n",
            "Iteration 84, loss = 0.50181526\n",
            "Iteration 85, loss = 0.49522015\n",
            "Iteration 86, loss = 0.48859090\n",
            "Iteration 87, loss = 0.48193858\n",
            "Iteration 88, loss = 0.47530579\n",
            "Iteration 89, loss = 0.46868631\n",
            "Iteration 90, loss = 0.46209154\n",
            "Iteration 91, loss = 0.45554031\n",
            "Iteration 92, loss = 0.44903126\n",
            "Iteration 93, loss = 0.44256186\n",
            "Iteration 94, loss = 0.43612950\n",
            "Iteration 95, loss = 0.42971294\n",
            "Iteration 96, loss = 0.42333311\n",
            "Iteration 97, loss = 0.41698531\n",
            "Iteration 98, loss = 0.41069819\n",
            "Iteration 99, loss = 0.40450287\n",
            "Iteration 100, loss = 0.39838715\n",
            "Iteration 101, loss = 0.39232717\n",
            "Iteration 102, loss = 0.38631500\n",
            "Iteration 103, loss = 0.38039654\n",
            "Iteration 104, loss = 0.37452836\n",
            "Iteration 105, loss = 0.36872373\n",
            "Iteration 106, loss = 0.36303128\n",
            "Iteration 107, loss = 0.35742855\n",
            "Iteration 108, loss = 0.35189415\n",
            "Iteration 109, loss = 0.34636482\n",
            "Iteration 110, loss = 0.34090246\n",
            "Iteration 111, loss = 0.33555454\n",
            "Iteration 112, loss = 0.33024183\n",
            "Iteration 113, loss = 0.32505857\n",
            "Iteration 114, loss = 0.32003522\n",
            "Iteration 115, loss = 0.31509765\n",
            "Iteration 116, loss = 0.31026741\n",
            "Iteration 117, loss = 0.30555993\n",
            "Iteration 118, loss = 0.30096048\n",
            "Iteration 119, loss = 0.29642405\n",
            "Iteration 120, loss = 0.29196778\n",
            "Iteration 121, loss = 0.28760344\n",
            "Iteration 122, loss = 0.28331230\n",
            "Iteration 123, loss = 0.27909563\n",
            "Iteration 124, loss = 0.27494968\n",
            "Iteration 125, loss = 0.27086936\n",
            "Iteration 126, loss = 0.26686434\n",
            "Iteration 127, loss = 0.26292815\n",
            "Iteration 128, loss = 0.25906554\n",
            "Iteration 129, loss = 0.25527574\n",
            "Iteration 130, loss = 0.25155983\n",
            "Iteration 131, loss = 0.24791493\n",
            "Iteration 132, loss = 0.24431997\n",
            "Iteration 133, loss = 0.24074103\n",
            "Iteration 134, loss = 0.23721756\n",
            "Iteration 135, loss = 0.23375717\n",
            "Iteration 136, loss = 0.23036685\n",
            "Iteration 137, loss = 0.22704308\n",
            "Iteration 138, loss = 0.22376934\n",
            "Iteration 139, loss = 0.22055271\n",
            "Iteration 140, loss = 0.21737711\n",
            "Iteration 141, loss = 0.21424224\n",
            "Iteration 142, loss = 0.21115746\n",
            "Iteration 143, loss = 0.20811684\n",
            "Iteration 144, loss = 0.20511560\n",
            "Iteration 145, loss = 0.20215260\n",
            "Iteration 146, loss = 0.19923008\n",
            "Iteration 147, loss = 0.19635498\n",
            "Iteration 148, loss = 0.19352308\n",
            "Iteration 149, loss = 0.19072471\n",
            "Iteration 150, loss = 0.18796421\n",
            "Iteration 151, loss = 0.18523985\n",
            "Iteration 152, loss = 0.18255221\n",
            "Iteration 153, loss = 0.17990557\n",
            "Iteration 154, loss = 0.17729776\n",
            "Iteration 155, loss = 0.17472943\n",
            "Iteration 156, loss = 0.17220499\n",
            "Iteration 157, loss = 0.16972109\n",
            "Iteration 158, loss = 0.16727510\n",
            "Iteration 159, loss = 0.16486726\n",
            "Iteration 160, loss = 0.16249641\n",
            "Iteration 161, loss = 0.16016201\n",
            "Iteration 162, loss = 0.15787642\n",
            "Iteration 163, loss = 0.15564221\n",
            "Iteration 164, loss = 0.15345849\n",
            "Iteration 165, loss = 0.15133040\n",
            "Iteration 166, loss = 0.14924382\n",
            "Iteration 167, loss = 0.14719603\n",
            "Iteration 168, loss = 0.14518795\n",
            "Iteration 169, loss = 0.14321849\n",
            "Iteration 170, loss = 0.14128264\n",
            "Iteration 171, loss = 0.13938111\n",
            "Iteration 172, loss = 0.13751236\n",
            "Iteration 173, loss = 0.13567480\n",
            "Iteration 174, loss = 0.13386477\n",
            "Iteration 175, loss = 0.13208377\n",
            "Iteration 176, loss = 0.13033250\n",
            "Iteration 177, loss = 0.12860076\n",
            "Iteration 178, loss = 0.12690621\n",
            "Iteration 179, loss = 0.12524526\n",
            "Iteration 180, loss = 0.12360379\n",
            "Iteration 181, loss = 0.12198472\n",
            "Iteration 182, loss = 0.12038898\n",
            "Iteration 183, loss = 0.11882771\n",
            "Iteration 184, loss = 0.11729246\n",
            "Iteration 185, loss = 0.11579641\n",
            "Iteration 186, loss = 0.11433018\n",
            "Iteration 187, loss = 0.11288829\n",
            "Iteration 188, loss = 0.11148311\n",
            "Iteration 189, loss = 0.11010835\n",
            "Iteration 190, loss = 0.10875977\n",
            "Iteration 191, loss = 0.10743438\n",
            "Iteration 192, loss = 0.10613247\n",
            "Iteration 193, loss = 0.10485140\n",
            "Iteration 194, loss = 0.10359282\n",
            "Iteration 195, loss = 0.10236132\n",
            "Iteration 196, loss = 0.10115095\n",
            "Iteration 197, loss = 0.09996327\n",
            "Iteration 198, loss = 0.09879987\n",
            "Iteration 199, loss = 0.09765723\n",
            "Iteration 200, loss = 0.09654102\n",
            "Iteration 201, loss = 0.09545416\n",
            "Iteration 202, loss = 0.09438430\n",
            "Iteration 203, loss = 0.09333504\n",
            "Iteration 204, loss = 0.09230274\n",
            "Iteration 205, loss = 0.09128961\n",
            "Iteration 206, loss = 0.09029580\n",
            "Iteration 207, loss = 0.08932358\n",
            "Iteration 208, loss = 0.08836977\n",
            "Iteration 209, loss = 0.08743233\n",
            "Iteration 210, loss = 0.08651319\n",
            "Iteration 211, loss = 0.08561290\n",
            "Iteration 212, loss = 0.08472887\n",
            "Iteration 213, loss = 0.08386131\n",
            "Iteration 214, loss = 0.08301280\n",
            "Iteration 215, loss = 0.08218169\n",
            "Iteration 216, loss = 0.08137089\n",
            "Iteration 217, loss = 0.08057581\n",
            "Iteration 218, loss = 0.07979956\n",
            "Iteration 219, loss = 0.07903650\n",
            "Iteration 220, loss = 0.07828471\n",
            "Iteration 221, loss = 0.07754699\n",
            "Iteration 222, loss = 0.07682227\n",
            "Iteration 223, loss = 0.07611057\n",
            "Iteration 224, loss = 0.07541106\n",
            "Iteration 225, loss = 0.07472569\n",
            "Iteration 226, loss = 0.07405558\n",
            "Iteration 227, loss = 0.07340063\n",
            "Iteration 228, loss = 0.07275607\n",
            "Iteration 229, loss = 0.07212144\n",
            "Iteration 230, loss = 0.07149748\n",
            "Iteration 231, loss = 0.07088328\n",
            "Iteration 232, loss = 0.07027955\n",
            "Iteration 233, loss = 0.06968574\n",
            "Iteration 234, loss = 0.06910071\n",
            "Iteration 235, loss = 0.06852585\n",
            "Iteration 236, loss = 0.06796401\n",
            "Iteration 237, loss = 0.06741187\n",
            "Iteration 238, loss = 0.06686832\n",
            "Iteration 239, loss = 0.06633338\n",
            "Iteration 240, loss = 0.06580624\n",
            "Iteration 241, loss = 0.06528755\n",
            "Iteration 242, loss = 0.06477735\n",
            "Iteration 243, loss = 0.06427470\n",
            "Iteration 244, loss = 0.06377733\n",
            "Iteration 245, loss = 0.06328616\n",
            "Iteration 246, loss = 0.06280149\n",
            "Iteration 247, loss = 0.06232470\n",
            "Iteration 248, loss = 0.06185754\n",
            "Iteration 249, loss = 0.06139818\n",
            "Iteration 250, loss = 0.06094510\n",
            "Iteration 251, loss = 0.06049928\n",
            "Iteration 252, loss = 0.06007957\n",
            "Iteration 253, loss = 0.05966787\n",
            "Iteration 254, loss = 0.05925800\n",
            "Iteration 255, loss = 0.05885037\n",
            "Iteration 256, loss = 0.05844729\n",
            "Iteration 257, loss = 0.05804729\n",
            "Iteration 258, loss = 0.05765152\n",
            "Iteration 259, loss = 0.05725982\n",
            "Iteration 260, loss = 0.05687248\n",
            "Iteration 261, loss = 0.05649955\n",
            "Iteration 262, loss = 0.05613544\n",
            "Iteration 263, loss = 0.05577129\n",
            "Iteration 264, loss = 0.05539857\n",
            "Iteration 265, loss = 0.05502016\n",
            "Iteration 266, loss = 0.05464212\n",
            "Iteration 267, loss = 0.05426480\n",
            "Iteration 268, loss = 0.05388626\n",
            "Iteration 269, loss = 0.05350667\n",
            "Iteration 270, loss = 0.05312725\n",
            "Iteration 271, loss = 0.05274792\n",
            "Iteration 272, loss = 0.05236972\n",
            "Iteration 273, loss = 0.05199308\n",
            "Iteration 274, loss = 0.05161886\n",
            "Iteration 275, loss = 0.05125838\n",
            "Iteration 276, loss = 0.05090305\n",
            "Iteration 277, loss = 0.05055146\n",
            "Iteration 278, loss = 0.05020397\n",
            "Iteration 279, loss = 0.04989753\n",
            "Iteration 280, loss = 0.04961023\n",
            "Iteration 281, loss = 0.04932592\n",
            "Iteration 282, loss = 0.04904421\n",
            "Iteration 283, loss = 0.04876498\n",
            "Iteration 284, loss = 0.04848793\n",
            "Iteration 285, loss = 0.04820822\n",
            "Iteration 286, loss = 0.04792226\n",
            "Iteration 287, loss = 0.04763113\n",
            "Iteration 288, loss = 0.04735120\n",
            "Iteration 289, loss = 0.04707635\n",
            "Iteration 290, loss = 0.04680326\n",
            "Iteration 291, loss = 0.04653314\n",
            "Iteration 292, loss = 0.04625798\n",
            "Iteration 293, loss = 0.04598320\n",
            "Iteration 294, loss = 0.04570918\n",
            "Iteration 295, loss = 0.04543558\n",
            "Iteration 296, loss = 0.04516410\n",
            "Iteration 297, loss = 0.04489480\n",
            "Iteration 298, loss = 0.04462788\n",
            "Iteration 299, loss = 0.04436334\n",
            "Iteration 300, loss = 0.04410269\n",
            "Iteration 301, loss = 0.04384405\n",
            "Iteration 302, loss = 0.04358655\n",
            "Iteration 303, loss = 0.04332982\n",
            "Iteration 304, loss = 0.04306719\n",
            "Iteration 305, loss = 0.04280693\n",
            "Iteration 306, loss = 0.04254932\n",
            "Iteration 307, loss = 0.04231579\n",
            "Iteration 308, loss = 0.04208819\n",
            "Iteration 309, loss = 0.04186304\n",
            "Iteration 310, loss = 0.04163950\n",
            "Iteration 311, loss = 0.04141814\n",
            "Iteration 312, loss = 0.04119930\n",
            "Iteration 313, loss = 0.04098311\n",
            "Iteration 314, loss = 0.04076992\n",
            "Iteration 315, loss = 0.04058054\n",
            "Iteration 316, loss = 0.04039118\n",
            "Iteration 317, loss = 0.04019967\n",
            "Iteration 318, loss = 0.04000607\n",
            "Iteration 319, loss = 0.03981136\n",
            "Iteration 320, loss = 0.03961572\n",
            "Iteration 321, loss = 0.03942058\n",
            "Iteration 322, loss = 0.03922481\n",
            "Iteration 323, loss = 0.03902852\n",
            "Iteration 324, loss = 0.03883094\n",
            "Iteration 325, loss = 0.03863260\n",
            "Iteration 326, loss = 0.03844971\n",
            "Iteration 327, loss = 0.03827042\n",
            "Iteration 328, loss = 0.03808961\n",
            "Iteration 329, loss = 0.03791083\n",
            "Iteration 330, loss = 0.03773236\n",
            "Iteration 331, loss = 0.03755367\n",
            "Iteration 332, loss = 0.03737490\n",
            "Iteration 333, loss = 0.03719752\n",
            "Iteration 334, loss = 0.03702031\n",
            "Iteration 335, loss = 0.03684335\n",
            "Iteration 336, loss = 0.03666829\n",
            "Iteration 337, loss = 0.03649275\n",
            "Iteration 338, loss = 0.03631789\n",
            "Iteration 339, loss = 0.03614362\n",
            "Iteration 340, loss = 0.03596068\n",
            "Iteration 341, loss = 0.03577339\n",
            "Iteration 342, loss = 0.03558420\n",
            "Iteration 343, loss = 0.03539472\n",
            "Iteration 344, loss = 0.03521215\n",
            "Iteration 345, loss = 0.03502659\n",
            "Iteration 346, loss = 0.03483805\n",
            "Iteration 347, loss = 0.03464960\n",
            "Iteration 348, loss = 0.03446485\n",
            "Iteration 349, loss = 0.03427983\n",
            "Iteration 350, loss = 0.03409251\n",
            "Iteration 351, loss = 0.03390606\n",
            "Iteration 352, loss = 0.03372071\n",
            "Iteration 353, loss = 0.03353898\n",
            "Iteration 354, loss = 0.03335633\n",
            "Iteration 355, loss = 0.03317115\n",
            "Iteration 356, loss = 0.03298965\n",
            "Iteration 357, loss = 0.03280797\n",
            "Iteration 358, loss = 0.03262827\n",
            "Iteration 359, loss = 0.03245120\n",
            "Iteration 360, loss = 0.03227300\n",
            "Iteration 361, loss = 0.03210693\n",
            "Iteration 362, loss = 0.03194403\n",
            "Iteration 363, loss = 0.03178131\n",
            "Iteration 364, loss = 0.03161894\n",
            "Iteration 365, loss = 0.03145710\n",
            "Iteration 366, loss = 0.03129949\n",
            "Iteration 367, loss = 0.03114116\n",
            "Iteration 368, loss = 0.03098072\n",
            "Iteration 369, loss = 0.03082776\n",
            "Iteration 370, loss = 0.03067446\n",
            "Iteration 371, loss = 0.03051940\n",
            "Iteration 372, loss = 0.03036753\n",
            "Iteration 373, loss = 0.03021614\n",
            "Iteration 374, loss = 0.03006463\n",
            "Iteration 375, loss = 0.02993381\n",
            "Iteration 376, loss = 0.02980441\n",
            "Iteration 377, loss = 0.02967714\n",
            "Iteration 378, loss = 0.02954967\n",
            "Iteration 379, loss = 0.02942355\n",
            "Iteration 380, loss = 0.02929712\n",
            "Iteration 381, loss = 0.02917117\n",
            "Iteration 382, loss = 0.02904570\n",
            "Iteration 383, loss = 0.02892771\n",
            "Iteration 384, loss = 0.02880694\n",
            "Iteration 385, loss = 0.02868269\n",
            "Iteration 386, loss = 0.02856329\n",
            "Iteration 387, loss = 0.02844378\n",
            "Iteration 388, loss = 0.02832243\n",
            "Iteration 389, loss = 0.02820246\n",
            "Iteration 390, loss = 0.02808279\n",
            "Iteration 391, loss = 0.02796500\n",
            "Iteration 392, loss = 0.02784697\n",
            "Iteration 393, loss = 0.02773330\n",
            "Iteration 394, loss = 0.02761704\n",
            "Iteration 395, loss = 0.02750080\n",
            "Iteration 396, loss = 0.02738456\n",
            "Iteration 397, loss = 0.02726621\n",
            "Iteration 398, loss = 0.02713242\n",
            "Iteration 399, loss = 0.02700604\n",
            "Iteration 400, loss = 0.02688631\n",
            "Iteration 401, loss = 0.02676608\n",
            "Iteration 402, loss = 0.02664360\n",
            "Iteration 403, loss = 0.02652382\n",
            "Iteration 404, loss = 0.02640246\n",
            "Iteration 405, loss = 0.02628196\n",
            "Iteration 406, loss = 0.02615850\n",
            "Iteration 407, loss = 0.02603775\n",
            "Iteration 408, loss = 0.02591358\n",
            "Iteration 409, loss = 0.02578691\n",
            "Iteration 410, loss = 0.02566284\n",
            "Iteration 411, loss = 0.02553562\n",
            "Iteration 412, loss = 0.02540881\n",
            "Iteration 413, loss = 0.02527900\n",
            "Iteration 414, loss = 0.02515112\n",
            "Iteration 415, loss = 0.02502289\n",
            "Iteration 416, loss = 0.02489374\n",
            "Iteration 417, loss = 0.02476426\n",
            "Iteration 418, loss = 0.02463292\n",
            "Iteration 419, loss = 0.02450228\n",
            "Iteration 420, loss = 0.02437094\n",
            "Iteration 421, loss = 0.02423937\n",
            "Iteration 422, loss = 0.02410880\n",
            "Iteration 423, loss = 0.02397606\n",
            "Iteration 424, loss = 0.02384386\n",
            "Iteration 425, loss = 0.02371184\n",
            "Iteration 426, loss = 0.02358308\n",
            "Iteration 427, loss = 0.02345166\n",
            "Iteration 428, loss = 0.02331737\n",
            "Iteration 429, loss = 0.02318632\n",
            "Iteration 430, loss = 0.02305520\n",
            "Iteration 431, loss = 0.02292526\n",
            "Iteration 432, loss = 0.02279264\n",
            "Iteration 433, loss = 0.02266406\n",
            "Iteration 434, loss = 0.02253355\n",
            "Iteration 435, loss = 0.02240210\n",
            "Iteration 436, loss = 0.02227240\n",
            "Iteration 437, loss = 0.02214255\n",
            "Iteration 438, loss = 0.02201472\n",
            "Iteration 439, loss = 0.02188473\n",
            "Iteration 440, loss = 0.02175853\n",
            "Iteration 441, loss = 0.02162961\n",
            "Iteration 442, loss = 0.02150377\n",
            "Iteration 443, loss = 0.02137710\n",
            "Iteration 444, loss = 0.02124766\n",
            "Iteration 445, loss = 0.02112672\n",
            "Iteration 446, loss = 0.02100241\n",
            "Iteration 447, loss = 0.02087510\n",
            "Iteration 448, loss = 0.02074646\n",
            "Iteration 449, loss = 0.02062453\n",
            "Iteration 450, loss = 0.02050017\n",
            "Iteration 451, loss = 0.02037477\n",
            "Iteration 452, loss = 0.02025620\n",
            "Iteration 453, loss = 0.02013456\n",
            "Iteration 454, loss = 0.02000957\n",
            "Iteration 455, loss = 0.01988903\n",
            "Iteration 456, loss = 0.01977023\n",
            "Iteration 457, loss = 0.01964941\n",
            "Iteration 458, loss = 0.01952758\n",
            "Iteration 459, loss = 0.01940971\n",
            "Iteration 460, loss = 0.01929195\n",
            "Iteration 461, loss = 0.01917292\n",
            "Iteration 462, loss = 0.01905623\n",
            "Iteration 463, loss = 0.01893922\n",
            "Iteration 464, loss = 0.01882200\n",
            "Iteration 465, loss = 0.01870670\n",
            "Iteration 466, loss = 0.01859138\n",
            "Iteration 467, loss = 0.01847777\n",
            "Iteration 468, loss = 0.01836337\n",
            "Iteration 469, loss = 0.01825011\n",
            "Iteration 470, loss = 0.01813591\n",
            "Iteration 471, loss = 0.01802738\n",
            "Iteration 472, loss = 0.01791582\n",
            "Iteration 473, loss = 0.01780125\n",
            "Iteration 474, loss = 0.01769122\n",
            "Iteration 475, loss = 0.01758113\n",
            "Iteration 476, loss = 0.01747097\n",
            "Iteration 477, loss = 0.01736085\n",
            "Iteration 478, loss = 0.01725323\n",
            "Iteration 479, loss = 0.01714395\n",
            "Iteration 480, loss = 0.01703872\n",
            "Iteration 481, loss = 0.01693145\n",
            "Iteration 482, loss = 0.01682223\n",
            "Iteration 483, loss = 0.01672035\n",
            "Iteration 484, loss = 0.01661597\n",
            "Iteration 485, loss = 0.01650808\n",
            "Iteration 486, loss = 0.01640187\n",
            "Iteration 487, loss = 0.01629944\n",
            "Iteration 488, loss = 0.01619487\n",
            "Iteration 489, loss = 0.01609129\n",
            "Iteration 490, loss = 0.01598808\n",
            "Iteration 491, loss = 0.01588650\n",
            "Iteration 492, loss = 0.01578336\n",
            "Iteration 493, loss = 0.01568183\n",
            "Iteration 494, loss = 0.01558167\n",
            "Iteration 495, loss = 0.01548081\n",
            "Iteration 496, loss = 0.01538229\n",
            "Iteration 497, loss = 0.01528180\n",
            "Iteration 498, loss = 0.01518363\n",
            "Iteration 499, loss = 0.01508488\n",
            "Iteration 500, loss = 0.01498646\n",
            "Iteration 501, loss = 0.01488943\n",
            "Iteration 502, loss = 0.01479089\n",
            "Iteration 503, loss = 0.01469655\n",
            "Iteration 504, loss = 0.01460014\n",
            "Iteration 505, loss = 0.01450126\n",
            "Iteration 506, loss = 0.01440745\n",
            "Iteration 507, loss = 0.01431184\n",
            "Iteration 508, loss = 0.01421636\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    }
  ]
}